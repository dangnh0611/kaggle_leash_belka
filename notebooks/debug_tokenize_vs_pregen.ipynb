{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ce793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers.models import WordLevel, BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace,Split,ByteLevel, WhitespaceSplit\n",
    "from tokenizers.normalizers import Lowercase, NFKC\n",
    "import os\n",
    "import polars as pl\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordLevelTrainer, WordPieceTrainer\n",
    "import gc\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "import mapply\n",
    "from collections import Counter\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from functools import partial\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af76eed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.842148938216269 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (98_415_610, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>molecule</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;C#CCOc1ccc(CNc…</td></tr><tr><td>&quot;C#CCOc1ccc(CNc…</td></tr><tr><td>&quot;C#CCOc1ccc(CNc…</td></tr><tr><td>&quot;C#CCOc1ccc(CNc…</td></tr><tr><td>&quot;C#CCOc1ccc(CNc…</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;[N-]=[N+]=NCCC…</td></tr><tr><td>&quot;[N-]=[N+]=NCCC…</td></tr><tr><td>&quot;[N-]=[N+]=NCCC…</td></tr><tr><td>&quot;[N-]=[N+]=NCCC…</td></tr><tr><td>&quot;[N-]=[N+]=NCCC…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (98_415_610, 1)\n",
       "┌───────────────────────────────────┐\n",
       "│ molecule                          │\n",
       "│ ---                               │\n",
       "│ str                               │\n",
       "╞═══════════════════════════════════╡\n",
       "│ C#CCOc1ccc(CNc2nc(NCC3CCCN3c3ccc… │\n",
       "│ C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3… │\n",
       "│ C#CCOc1ccc(CNc2nc(NCc3ccc(OCC#C)… │\n",
       "│ C#CCOc1ccc(CNc2nc(NCCNC(=O)C(=C)… │\n",
       "│ C#CCOc1ccc(CNc2nc(NCC(=O)NCC=C)n… │\n",
       "│ …                                 │\n",
       "│ [N-]=[N+]=NCCC[C@H](Nc1nc(Nc2noc… │\n",
       "│ [N-]=[N+]=NCCC[C@H](Nc1nc(Nc2nnc… │\n",
       "│ [N-]=[N+]=NCCC[C@H](Nc1nc(Nc2nnc… │\n",
       "│ [N-]=[N+]=NCCC[C@H](Nc1nc(Nc2nnn… │\n",
       "│ [N-]=[N+]=NCCC[C@H](Nc1nc(Nc2noc… │\n",
       "└───────────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pl.scan_csv('/home/dangnh36/datasets/competitions/leash_belka/processed/train_v2.csv').select(\n",
    "        pl.col('molecule'),\n",
    "#         pl.col('bb1', 'bb2', 'bb3').cast(pl.UInt16),\n",
    "#         pl.col('BRD4', 'HSA', 'sEH').cast(pl.UInt8),\n",
    "    ).collect()\n",
    "print(train_df.estimated_size('gb'), 'GB')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c110f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06128192972391844 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (878_022, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>molecule</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;C#CCCC[C@H](Nc…</td></tr><tr><td>&quot;C#CCCC[C@H](Nc…</td></tr><tr><td>&quot;C#CCCC[C@H](Nc…</td></tr><tr><td>&quot;C#CCCC[C@H](Nc…</td></tr><tr><td>&quot;C#CCCC[C@H](Nc…</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;Cn1ncc2cc(Nc3n…</td></tr><tr><td>&quot;[N-]=[N+]=NCCC…</td></tr><tr><td>&quot;COC(=O)c1ccnc(…</td></tr><tr><td>&quot;COC1CCC(CCNc2n…</td></tr><tr><td>&quot;[N-]=[N+]=NCCC…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (878_022, 1)\n",
       "┌───────────────────────────────────┐\n",
       "│ molecule                          │\n",
       "│ ---                               │\n",
       "│ str                               │\n",
       "╞═══════════════════════════════════╡\n",
       "│ C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2… │\n",
       "│ C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2… │\n",
       "│ C#CCCC[C@H](Nc1nc(NCC2(O)CCCC2(C… │\n",
       "│ C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2… │\n",
       "│ C#CCCC[C@H](Nc1nc(NCC2CCC(SC)CC2… │\n",
       "│ …                                 │\n",
       "│ Cn1ncc2cc(Nc3nc(Nc4nncs4)nc(N[C@… │\n",
       "│ [N-]=[N+]=NCCC[C@H](Nc1nc(NCC2CC… │\n",
       "│ COC(=O)c1ccnc(Nc2nc(Nc3noc4ccc(F… │\n",
       "│ COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc3… │\n",
       "│ [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cc… │\n",
       "└───────────────────────────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pl.scan_csv('/home/dangnh36/datasets/competitions/leash_belka/processed/test_v4.csv').select(\n",
    "        pl.col('molecule'),\n",
    "#         pl.col('bb1', 'bb2', 'bb3').cast(pl.UInt16),\n",
    "        # pl.col('BRD4', 'HSA', 'sEH').cast(pl.UInt8),\n",
    "    ).collect()\n",
    "print(test_df.estimated_size('gb'), 'GB')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6d3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3262442d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZER: PreTrainedTokenizerFast(name_or_path='/home/dangnh36/datasets/competitions/leash_belka/processed/tokenizer_v2/smiles_char/', vocab_size=44, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[BOS]', 'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"Br\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t14: AddedToken(\"Cl\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t32: AddedToken(\"@@\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t43: AddedToken(\"[Dy]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "}\n",
      "VOCAB SIZE: 44\n",
      "VOCAB: {'O': 10, '+': 27, '(': 30, '9': 42, 'c': 19, '/': 25, '1': 34, 'C': 8, 'S': 12, '4': 37, '[BOS]': 5, '6': 39, 'H': 11, '[EOS]': 6, '#': 24, '[SEP]': 4, '-': 26, '[': 28, '=': 23, '7': 40, '8': 41, 'B': 15, 'n': 20, '[CLS]': 2, '[Dy]': 43, 'Br': 7, '@': 33, 'N': 9, 'i': 21, 'I': 16, '5': 38, ']': 29, '@@': 32, '[PAD]': 0, ')': 31, '[MASK]': 3, 'Cl': 14, 'F': 13, '.': 22, '2': 35, '3': 36, '[UNK]': 1, 'o': 18, 's': 17}\n",
      "SPECIAL TOKENS: {'bos_token': '[BOS]', 'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/dangnh36/datasets/competitions/leash_belka/processed/tokenizer_v2/smiles_char/')\n",
    "print('TOKENIZER:', tokenizer)\n",
    "print('VOCAB SIZE:', tokenizer.vocab_size)\n",
    "print('VOCAB:', tokenizer.get_vocab())\n",
    "print('SPECIAL TOKENS:', tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01637a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "607edbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bd14d761634af1880eebb3ee7610da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "ds = load_from_disk(\n",
    "    '/home/dangnh36/datasets/competitions/leash_belka/processed/hf/datasets/train/',\n",
    "    keep_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6cdcbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smiles_list = train_df[:123456, 'molecule'].to_list()\n",
    "smiles_list = ds[:123456]['smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51c6dc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123456"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smiles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72884d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2,  8, 24,  ...,  0,  0,  0],\n",
       "        [ 2,  8, 24,  ...,  0,  0,  0],\n",
       "        [ 2,  8, 24,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 2,  8, 24,  ...,  0,  0,  0],\n",
       "        [ 2,  8, 24,  ...,  0,  0,  0],\n",
       "        [ 2,  8, 24,  ...,  0,  0,  0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'length': tensor([115, 115, 115,  ..., 115, 115, 115])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = tokenizer(\n",
    "            [f'[CLS]{s}' for s in smiles_list],\n",
    "            add_special_tokens=True,\n",
    "            padding='longest',\n",
    "            truncation=False,\n",
    "            max_length=2048,\n",
    "            is_split_into_words=False,\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_special_tokens_mask=False,\n",
    "            return_length=True,\n",
    "            verbose=True)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a887d3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123456, 139)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2, 18,  2, ...,  0,  0,  0],\n",
       "       [ 2, 18,  2, ...,  0,  0,  0],\n",
       "       [ 2, 18,  2, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 2, 18,  2, ...,  0,  0,  0],\n",
       "       [ 2, 18,  2, ...,  0,  0,  0],\n",
       "       [ 2, 18,  2, ...,  0,  0,  0]], dtype=uint8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_v2 = np.load('/home/dangnh36/datasets/competitions/leash_belka/processed/features/smiles_char/train.npy', mmap_mode='r')\n",
    "features_v2 = np.array(features_v2[:123456])\n",
    "print(features_v2.shape)\n",
    "features_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c2eb2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 10, '+': 27, '(': 30, '9': 42, 'c': 19, '/': 25, '1': 34, 'C': 8, 'S': 12, '4': 37, '[BOS]': 5, '6': 39, 'H': 11, '[EOS]': 6, '#': 24, '[SEP]': 4, '-': 26, '[': 28, '=': 23, '7': 40, '8': 41, 'B': 15, 'n': 20, '[CLS]': 2, '[Dy]': 43, 'Br': 7, '@': 33, 'N': 9, 'i': 21, 'I': 16, '5': 38, ']': 29, '@@': 32, '[PAD]': 0, ')': 31, '[MASK]': 3, 'Cl': 14, 'F': 13, '.': 22, '2': 35, '3': 36, '[UNK]': 1, 'o': 18, 's': 17}\n"
     ]
    }
   ],
   "source": [
    "vocab_v1 = tokenizer.get_vocab()\n",
    "print(vocab_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bac9b44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, 'Br': 1, 'C': 2, 'N': 3, 'O': 4, 'H': 5, 'S': 6, 'F': 7, 'Cl': 8, 'B': 9, 'I': 10, 's': 11, 'o': 12, 'c': 13, 'n': 14, 'i': 15, '.': 16, '=': 17, '#': 18, '/': 19, '-': 20, '+': 21, '[': 22, ']': 23, '(': 24, ')': 25, '@@': 26, '@': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36, '[Dy]': 37, '[UNK]': 38}\n"
     ]
    }
   ],
   "source": [
    "vocab_v2 = {'[PAD]': 0, 'Br': 1, 'C': 2, 'N': 3, 'O': 4, 'H': 5, 'S': 6, 'F': 7, 'Cl': 8, 'B': 9, 'I': 10, 's': 11, 'o': 12, 'c': 13, 'n': 14, 'i': 15, '.': 16, '=': 17, '#': 18, '/': 19, '-': 20, '+': 21, '[': 22, ']': 23, '(': 24, ')': 25, '@@': 26, '@': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36, '[Dy]': 37, '[UNK]': 38}\n",
    "print(vocab_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ccea44fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 4,\n",
       " 27: 21,\n",
       " 30: 24,\n",
       " 42: 36,\n",
       " 19: 13,\n",
       " 25: 19,\n",
       " 34: 28,\n",
       " 8: 2,\n",
       " 12: 6,\n",
       " 37: 31,\n",
       " 5: -999,\n",
       " 39: 33,\n",
       " 11: 5,\n",
       " 6: -999,\n",
       " 24: 18,\n",
       " 4: -999,\n",
       " 26: 20,\n",
       " 28: 22,\n",
       " 23: 17,\n",
       " 40: 34,\n",
       " 41: 35,\n",
       " 15: 9,\n",
       " 20: 14,\n",
       " 2: -999,\n",
       " 43: 37,\n",
       " 7: 1,\n",
       " 33: 27,\n",
       " 9: 3,\n",
       " 21: 15,\n",
       " 16: 10,\n",
       " 38: 32,\n",
       " 29: 23,\n",
       " 32: 26,\n",
       " 0: 0,\n",
       " 31: 25,\n",
       " 3: -999,\n",
       " 14: 8,\n",
       " 13: 7,\n",
       " 22: 16,\n",
       " 35: 29,\n",
       " 36: 30,\n",
       " 1: 38,\n",
       " 18: 12,\n",
       " 17: 11}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_v2_inv = {v: k for k, v in vocab_v2.items()}\n",
    "v1_to_v2 = {v: vocab_v2[k] if k in vocab_v2 else -999 for k, v in vocab_v1.items()}\n",
    "v1_to_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f024471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10: 4, 27: 21, 30: 24, 42: 36, 19: 13, 25: 19, 34: 28, 8: 2, 12: 6, 37: 31, 5: -999, 39: 33, 11: 5, 6: -999, 24: 18, 4: -999, 26: 20, 28: 22, 23: 17, 40: 34, 41: 35, 15: 9, 20: 14, 2: -999, 43: 37, 7: 1, 33: 27, 9: 3, 21: 15, 16: 10, 38: 32, 29: 23, 32: 26, 0: 0, 31: 25, 3: -999, 14: 8, 13: 7, 22: 16, 35: 29, 36: 30, 1: 38, 18: 12, 17: 11}\n"
     ]
    }
   ],
   "source": [
    "print(v1_to_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503203b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf17b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4336266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123456it [00:12, 9693.81it/s] \n"
     ]
    }
   ],
   "source": [
    "def compare(features_v1, features_v2, pads_v1):\n",
    "    for i, (row1, row2, pad1) in tqdm(enumerate(zip(features_v1, features_v2, pads_v1))):\n",
    "        pad1 = pad1[1:].cpu().numpy()\n",
    "        assert row1[0] == 2\n",
    "        assert pad1[0] == True\n",
    "        row1 = row1[1:].cpu().numpy()\n",
    "        row1_new = np.array([v1_to_v2[e] for e in row1])\n",
    "        non_zero1 = np.where(row1_new)[0]\n",
    "        non_zero2 = np.where(row2)[0]\n",
    "        row1_new = row1_new[non_zero1]\n",
    "        row2 = row2[non_zero2]\n",
    "#         print(row1_new)\n",
    "#         print(row2)\n",
    "        assert np.all(row1_new == row2)\n",
    "        assert np.all(np.where(pad1)[0] == non_zero1)\n",
    "\n",
    "compare(ret['input_ids'].long(), features_v2, ret['attention_mask'].bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969523a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7126d997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8ac23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38079bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcc409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeaef4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eabc12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9918b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce59f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071bf320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
