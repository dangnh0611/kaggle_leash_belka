{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8754e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "import sys\n",
    "import polars as pl\n",
    "import sklearn\n",
    "import shap\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from rdkit.Chem import DataStructs\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import math\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4e9cb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_IDXS = [0, 1, 2]\n",
    "CLASS_IDX = 0\n",
    "# SAVE_DIR = '/home/dangnh36/datasets/competitions/leash_belka/processed/feature_importance/public_nonshare-mlp-ecfp6-bb_grid-scaffold_5_8_2_3_shuffle_5.8M'\n",
    "\n",
    "SAVE_DIR = '/home/dangnh36/datasets/competitions/leash_belka/processed/feature_importance/public_nonshare-mlp-ecfp6-bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M'\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "DEVICE = torch.device('cuda:2')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70d6bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00981266051530838 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28584/361753753.py:8: DeprecationWarning: `count` is deprecated. It has been renamed to `len`.\n",
      "  test_df.group_by('mol_group').count().sort('mol_group')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>mol_group</th><th>count</th></tr><tr><td>i64</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>369039</td></tr><tr><td>1</td><td>486390</td></tr><tr><td>2</td><td>11271</td></tr><tr><td>3</td><td>11322</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 2)\n",
       "┌───────────┬────────┐\n",
       "│ mol_group ┆ count  │\n",
       "│ ---       ┆ ---    │\n",
       "│ i64       ┆ u32    │\n",
       "╞═══════════╪════════╡\n",
       "│ 0         ┆ 369039 │\n",
       "│ 1         ┆ 486390 │\n",
       "│ 2         ┆ 11271  │\n",
       "│ 3         ┆ 11322  │\n",
       "└───────────┴────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pl.scan_csv('/home/dangnh36/datasets/competitions/leash_belka/processed/test_v4.csv').with_row_index('index').select(\n",
    "#         pl.col('molecule'),\n",
    "        pl.col('index', 'mol_group'),\n",
    "#         pl.col('bb1', 'bb2', 'bb3').cast(pl.UInt16),\n",
    "        # pl.col('BRD4', 'HSA', 'sEH').cast(pl.UInt8),\n",
    "    ).collect()\n",
    "print(test_df.estimated_size('gb'), 'GB')\n",
    "test_df.group_by('mol_group').count().sort('mol_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f759f93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (878_022, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>index</th><th>mol_group</th></tr><tr><td>u32</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>2</td></tr><tr><td>1</td><td>2</td></tr><tr><td>2</td><td>2</td></tr><tr><td>3</td><td>2</td></tr><tr><td>4</td><td>2</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>878017</td><td>0</td></tr><tr><td>878018</td><td>0</td></tr><tr><td>878019</td><td>0</td></tr><tr><td>878020</td><td>0</td></tr><tr><td>878021</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (878_022, 2)\n",
       "┌────────┬───────────┐\n",
       "│ index  ┆ mol_group │\n",
       "│ ---    ┆ ---       │\n",
       "│ u32    ┆ i64       │\n",
       "╞════════╪═══════════╡\n",
       "│ 0      ┆ 2         │\n",
       "│ 1      ┆ 2         │\n",
       "│ 2      ┆ 2         │\n",
       "│ 3      ┆ 2         │\n",
       "│ 4      ┆ 2         │\n",
       "│ …      ┆ …         │\n",
       "│ 878017 ┆ 0         │\n",
       "│ 878018 ┆ 0         │\n",
       "│ 878019 ┆ 0         │\n",
       "│ 878020 ┆ 0         │\n",
       "│ 878021 ┆ 0         │\n",
       "└────────┴───────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d1aed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "878022"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "369039 + 486390 + 11271 + 11322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f14f8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "[(98415610,), (369039,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (486390,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11271,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,), (11322,)]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_LEN = 98_415_610\n",
    "idxs = [np.arange(0, TRAIN_LEN, 1)]\n",
    "\n",
    "# ┌───────────┬────────┐\n",
    "# │ mol_group ┆ count  │\n",
    "# │ ---       ┆ ---    │\n",
    "# │ i64       ┆ u32    │\n",
    "# ╞═══════════╪════════╡\n",
    "# │ 0         ┆ 369039 │\n",
    "# │ 1         ┆ 486390 │\n",
    "# │ 2         ┆ 11271  │\n",
    "# │ 3         ┆ 11322  │\n",
    "# └───────────┴────────┘\n",
    "\n",
    "group_ids = [0, 1, 2, 3]\n",
    "group_weights = [1, 25, 280, 280]\n",
    "for group_id, group_weight in zip(group_ids, group_weights):\n",
    "    print(group_id)\n",
    "    group_idxs = test_df.filter(pl.col('mol_group') == group_id)['index'].to_numpy()\n",
    "    idxs.extend([group_idxs] * group_weight)\n",
    "\n",
    "print([e.shape for e in idxs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dfe2691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117270439,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = np.concatenate(idxs, axis = 0)\n",
    "idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14fb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299fd799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11271, 2048)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.load('/home/dangnh36/datasets/competitions/leash_belka/processed/features/ecfp6/test.npy', mmap_mode='r')\n",
    "nonshare_test_idxs = test_df.filter(pl.col('mol_group') == 2)['index']\n",
    "test_features = test_data[nonshare_test_idxs]\n",
    "test_features = np.unpackbits(test_features, axis = -1)\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b40da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27833c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import nn\n",
    "import torch\n",
    "import logging\n",
    "from functools import partial\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "from src.modules import layers\n",
    "from src.modules.misc import get_act_fn\n",
    "\n",
    "NORMS = {'LN': nn.LayerNorm, 'BN': nn.BatchNorm1d}\n",
    "\n",
    "\n",
    "def arr_to_fp(arr):\n",
    "    bitstring=\"\".join(arr.astype(str))\n",
    "    fp = DataStructs.cDataStructs.CreateFromBitString(bitstring)\n",
    "    return fp\n",
    "\n",
    "class MlpModel(nn.Module):\n",
    "\n",
    "    def __init__(self, global_cfg, class_idx = None):\n",
    "        super().__init__()\n",
    "        cfg = global_cfg.model\n",
    "        self.class_idx = class_idx\n",
    "        self.mlp = layers.MLP(\n",
    "            cfg.in_dim,\n",
    "            cfg.mlp_chans + [3],\n",
    "            norm_layer=NORMS[cfg.norm] if cfg.norm is not None else None,\n",
    "            act_layer=get_act_fn(cfg.act),\n",
    "            dropout=cfg.dropout,\n",
    "            last_norm=False,\n",
    "            last_activation=False,\n",
    "            last_dropout=False)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        x = self.mlp(x.float())\n",
    "        if self.class_idx is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x[:, self.class_idx:self.class_idx+1]\n",
    "    \n",
    "    \n",
    "def get_mlp_model(weight_path = None, class_idx = None, device = DEVICE):\n",
    "    from types import SimpleNamespace\n",
    "    global_cfg = SimpleNamespace(**{})\n",
    "    model_cfg = SimpleNamespace(**{})\n",
    "    global_cfg.model = model_cfg\n",
    "\n",
    "    model_cfg.in_dim = 2048\n",
    "    model_cfg.mlp_chans = [512, 512]\n",
    "    model_cfg.norm = None\n",
    "    model_cfg.act = 'gelu'\n",
    "    model_cfg.dropout = 0.3\n",
    "    model = MlpModel(global_cfg, class_idx = class_idx)\n",
    "    if weight_path is not None:\n",
    "        ckpt = torch.load(weight_path, map_location = 'cpu')\n",
    "        ckpt = {k.replace('model.', '', 1): v for k, v in ckpt['state_dict'].items()}\n",
    "        model.load_state_dict(ckpt, strict=True)\n",
    "    model = model.to(DEVICE).eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_feature_importance(shap_values, feature_names = None):\n",
    "    assert shap_values.ndim == 2\n",
    "    if feature_names is not None:\n",
    "        assert len(feature_names) == shap_values.shape[1]\n",
    "    feat_importances = np.abs(shap_values).mean(0)\n",
    "    feat_importances_df = pd.DataFrame({'feature': feature_names, 'importance': feat_importances}).sort_values('importance', ascending=False)\n",
    "    return feat_importances_df\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def mlp_predict(x, model, class_idx = 0, bs = 2**14):\n",
    "    ret = np.zeros((x.shape[0],), dtype = np.float32)\n",
    "    for start in range(0, x.shape[0], bs):\n",
    "        inp = torch.from_numpy(x[start:start + bs]).float().to('cuda:2')\n",
    "        logit = model(inp)\n",
    "        logit = logit[:, class_idx].cpu().numpy()\n",
    "        ret[start:start + bs] = logit\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_sweep_ckpt_paths(root_dir):\n",
    "    ret = []\n",
    "    run_dirs = [os.path.join(root_dir, e) for e in os.listdir(root_dir)]\n",
    "    run_dirs = sorted([e for e in run_dirs if os.path.isdir(e)])\n",
    "    for run_dir in run_dirs:\n",
    "        sub_dirs = os.listdir(run_dir)\n",
    "        sub_dirs = [e for e in sub_dirs if 'fold_' in e]\n",
    "        assert len(sub_dirs) == 1\n",
    "        sub_dir = sub_dirs[0]\n",
    "        fold_idx = int(sub_dir.replace('fold_', ''))\n",
    "        ckpt_path = os.path.join(run_dir, sub_dir, 'ckpts', 'last.ckpt')\n",
    "        assert os.path.isfile(ckpt_path)\n",
    "        ret.append([fold_idx, ckpt_path])\n",
    "    ret.sort(key = lambda x: x[0])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "549bec40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/0_callbacks.early_stopping.patience=2,cv.fold_idx=0,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,tr_ETC_/fold_0/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/1_callbacks.early_stopping.patience=2,cv.fold_idx=1,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,tr_ETC_/fold_1/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/2_callbacks.early_stopping.patience=2,cv.fold_idx=2,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,tr_ETC_/fold_2/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/3_callbacks.early_stopping.patience=2,cv.fold_idx=3,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,tr_ETC_/fold_3/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/4_callbacks.early_stopping.patience=2,cv.fold_idx=4,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,tr_ETC_/fold_4/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/5_callbacks.early_stopping.patience=2,cv.fold_idx=5,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,tr_ETC_/fold_5/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/6_callbacks.early_stopping.patience=2,cv.fold_idx=6,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,tr_ETC_/fold_6/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/7_callbacks.early_stopping.patience=2,cv.fold_idx=7,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,tr_ETC_/fold_7/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/8_callbacks.early_stopping.patience=2,cv.fold_idx=8,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,tr_ETC_/fold_8/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/9_callbacks.early_stopping.patience=2,cv.fold_idx=9,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,tr_ETC_/fold_9/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/10_callbacks.early_stopping.patience=2,cv.fold_idx=10,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_10/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/11_callbacks.early_stopping.patience=2,cv.fold_idx=11,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_11/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/12_callbacks.early_stopping.patience=2,cv.fold_idx=12,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_12/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/13_callbacks.early_stopping.patience=2,cv.fold_idx=13,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_13/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/14_callbacks.early_stopping.patience=2,cv.fold_idx=14,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_14/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/15_callbacks.early_stopping.patience=2,cv.fold_idx=15,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_15/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/16_callbacks.early_stopping.patience=2,cv.fold_idx=16,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_16/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/17_callbacks.early_stopping.patience=2,cv.fold_idx=17,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_17/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/18_callbacks.early_stopping.patience=2,cv.fold_idx=18,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_18/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/19_callbacks.early_stopping.patience=2,cv.fold_idx=19,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_19/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/20_callbacks.early_stopping.patience=2,cv.fold_idx=20,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_20/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/21_callbacks.early_stopping.patience=2,cv.fold_idx=21,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_21/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/22_callbacks.early_stopping.patience=2,cv.fold_idx=22,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_22/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/23_callbacks.early_stopping.patience=2,cv.fold_idx=23,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_23/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/24_callbacks.early_stopping.patience=2,cv.fold_idx=24,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_24/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/25_callbacks.early_stopping.patience=2,cv.fold_idx=25,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_25/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/26_callbacks.early_stopping.patience=2,cv.fold_idx=26,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_26/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/27_callbacks.early_stopping.patience=2,cv.fold_idx=27,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_27/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/28_callbacks.early_stopping.patience=2,cv.fold_idx=28,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_28/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/29_callbacks.early_stopping.patience=2,cv.fold_idx=29,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_29/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/30_callbacks.early_stopping.patience=2,cv.fold_idx=30,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_30/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/31_callbacks.early_stopping.patience=2,cv.fold_idx=31,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_31/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/32_callbacks.early_stopping.patience=2,cv.fold_idx=32,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_32/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/33_callbacks.early_stopping.patience=2,cv.fold_idx=33,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_33/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/34_callbacks.early_stopping.patience=2,cv.fold_idx=34,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_34/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/35_callbacks.early_stopping.patience=2,cv.fold_idx=35,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_35/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/36_callbacks.early_stopping.patience=2,cv.fold_idx=36,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_36/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/37_callbacks.early_stopping.patience=2,cv.fold_idx=37,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_37/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/38_callbacks.early_stopping.patience=2,cv.fold_idx=38,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_38/ckpts/last.ckpt',\n",
       " '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/39_callbacks.early_stopping.patience=2,cv.fold_idx=39,cv.strategy=bb_grid-scaffold_gkf_5_8_2_2_shuffle_2.0M,exp=pregen_fp_mlp,exp_name=pregen-ecfp6-02_mlp,optim.lr=0.0005,optim.weight_decay=0.0,trainer.check_val_every_n_epoch=5,trainer.max_epochs=40,_ETC_/fold_39/ckpts/last.ckpt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ckpt_paths = get_sweep_ckpt_paths(\n",
    "#     '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-17/08-36-00.533707_pregen-ecfp6-02_mlp/') + \\\n",
    "#     get_sweep_ckpt_paths(\n",
    "#     '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-17/08-36-28.828748_pregen-ecfp6-02_mlp/')\n",
    "\n",
    "\n",
    "ckpt_paths = get_sweep_ckpt_paths(\n",
    "    '/home/dangnh36/projects/competitions/kaggle_leash_belka/src/outputs/train/multirun/06-14/15-25-27.097692_pregen-ecfp6-02_mlp/')\n",
    "\n",
    "ckpt_paths.sort(key = lambda x: x[0])\n",
    "ckpt_paths = [e[1] for e in ckpt_paths]\n",
    "ckpt_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194333f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38452de7",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea353f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 11271/11271 [00:16<00:00, 674.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11271"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fps = [arr_to_fp(fp) for fp in tqdm(test_features)]\n",
    "len(test_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4fe110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_clustering(fps, threshold = 0.6):\n",
    "    from rdkit import Chem, DataStructs\n",
    "    from rdkit.Chem import AllChem\n",
    "    from rdkit.ML.Cluster import Butina\n",
    "            \n",
    "    if isinstance(fps, np.ndarray):\n",
    "        fps = [arr_to_fp(fp) for fp in tqdm(fps)]\n",
    "    dists = []\n",
    "    nfps = len(fps)\n",
    "    for i in tqdm(range(1, nfps)):\n",
    "        sims = DataStructs.BulkTanimotoSimilarity(fps[i], fps[:i])\n",
    "        dists.extend([1 - x for x in sims])\n",
    "    clusters = Butina.ClusterData(dists,\n",
    "                               nfps,\n",
    "                               threshold,\n",
    "                               isDistData=True)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def pickup_cluster_elements(cluster_idxs, all_fps, n):\n",
    "    \"\"\"Pickup a diverse set of n samples within a cluster,\n",
    "    heuristically maximize pair-wise distance\"\"\"\n",
    "    centroid_idx = cluster_idxs[0]\n",
    "    cluster_fps = [all_fps[i] for i in cluster_idxs]\n",
    "    selected_idxs = [centroid_idx]\n",
    "    for _ in range(1, n):\n",
    "        dists = []\n",
    "        for selected_idx in selected_idxs:\n",
    "            sims = DataStructs.BulkTanimotoSimilarity(all_fps[selected_idx], cluster_fps)\n",
    "            dist = np.array([1 - x for x in sims])\n",
    "            dists.append(dist)\n",
    "        dists = np.array(dists)\n",
    "        dists = dists.mean(axis = 0)\n",
    "        assert dists.shape[0] == len(cluster_idxs) and len(dists.shape) == 1\n",
    "        dists[[cluster_idxs.index(e) for e in selected_idxs]] = -9999. # ignore\n",
    "        next_selected_idx = cluster_idxs[np.argmax(dists)]\n",
    "#         print(f'{_} -> {next_selected_idx} with cost {dists[np.argmax(dists)]} (min={np.min(dists)})')\n",
    "        selected_idxs.append(next_selected_idx)\n",
    "    return selected_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b960e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 11270/11270 [00:25<00:00, 444.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM CLUSTERS: 292\n",
      "{0: (10990, 382, 1045, 1370, 1447, 1502, 1619, 1659, 1689, 1703, 1708, 1709, 1710, 1712, 1714, 1717, 3697, 4360, 4685, 4934, 5018, 5023, 5027, 5686, 6349, 7675, 8338, 9664, 9989, 10121, 10238, 10308, 10322, 10327, 10328, 10329, 10331, 10608, 10610, 10612, 10614, 10615, 10617, 10619, 10620, 10621, 10622, 10623, 10625, 10628, 10630, 10635, 10636, 10639, 10642, 10644, 10645, 10646, 10647, 10648, 10649, 10650, 10651, 10652, 10653, 10654, 10655, 10656, 10657, 10658, 10659, 10660, 10661, 10662, 10663, 10664, 10669, 10670, 10676, 10679, 10682, 10693, 10695, 10704, 10705, 10722, 10724, 10725, 10726, 10727, 10729, 10730, 10732, 10733, 10734, 10735, 10740, 10744, 10746, 10749, 10754, 10757, 10760, 10767, 10768, 10769, 10770, 10771, 10772, 10773, 10774, 10775, 10776, 10777, 10778, 10779, 10780, 10781, 10782, 10783, 10784, 10785, 10786, 10787, 10788, 10789, 10790, 10791, 10792, 10793, 10794, 10795, 10796, 10797, 10798, 10799, 10801, 10803, 10804, 10808, 10810, 10812, 10814, 10815, 10816, 10818, 10820, 10823, 10829, 10834, 10838, 10840, 10842, 10843, 10846, 10849, 10861, 10863, 10870, 10871, 10873, 10874, 10877, 10878, 10880, 10885, 10887, 10889, 10890, 10891, 10892, 10893, 10894, 10895, 10896, 10897, 10898, 10899, 10900, 10901, 10902, 10903, 10904, 10905, 10906, 10907, 10908, 10909, 10910, 10911, 10912, 10913, 10915, 10917, 10919, 10922, 10924, 10925, 10926, 10927, 10929, 10930, 10931, 10932, 10933, 10934, 10935, 10936, 10937, 10938, 10939, 10940, 10941, 10942, 10943, 10944, 10945, 10946, 10947, 10949, 10950, 10951, 10952, 10953, 10954, 10956, 10957, 10958, 10959, 10960, 10961, 10962, 10963, 10964, 10965, 10966, 10967, 10968, 10969, 10970, 10971, 10972, 10973, 10974, 10975, 10976, 10977, 10978, 10979, 10980, 10981, 10982, 10983, 10984, 10985, 10986, 10987, 10988, 10989, 10991, 10992, 10993, 10994, 10995, 10996, 10997, 10998, 10999, 11000, 11002, 11006, 11008, 11009, 11010, 11011, 11013, 11021, 11023, 11025, 11027, 11029, 11030, 11031, 11033, 11038, 11041, 11042, 11043, 11044, 11045, 11046, 11047, 11048, 11049, 11050, 11051, 11052, 11053, 11054, 11055, 11057, 11059, 11060, 11061, 11062, 11063, 11065, 11066, 11068, 11070, 11073, 11074, 11075, 11077, 11080, 11083, 11084, 11085, 11086, 11087, 11088, 11089, 11090, 11091, 11092, 11093, 11098, 11106, 11108, 11109, 11112, 11116, 11124, 11129, 11130, 11131, 11136, 11138, 11140, 11141, 11143, 11145, 11146, 11147, 11148, 11151, 11153, 11154, 11156, 11157, 11158, 11159, 11160, 11162, 11163, 11164, 11165, 11169, 11172, 11174, 11177, 11185, 11191, 11199, 11201, 11202, 11205, 11208, 11213, 11218, 11224, 11240, 11241, 11254, 11258, 11260, 11261, 11263, 11264, 11265, 11266, 11268, 11269), 1: (8152, 6, 65, 196, 478, 539, 653, 669, 704, 728, 754, 781, 806, 833, 856, 857, 858, 859, 860, 861, 866, 891, 923, 951, 988, 1022, 1043, 1079, 1099, 1115, 1141, 1182, 1202, 1230, 1251, 1276, 1316, 1332, 1367, 1391, 1417, 1444, 1469, 1496, 1519, 1520, 1522, 1523, 1524, 1529, 1614, 1651, 1685, 1706, 1762, 1778, 1804, 1845, 1865, 1893, 1914, 1939, 1979, 2185, 2528, 2658, 2693, 2717, 2743, 2770, 2795, 2822, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2855, 2880, 2912, 2940, 2962, 2977, 3011, 3032, 3049, 3068, 3088, 3104, 3115, 3130, 3147, 3171, 3191, 3219, 3240, 3265, 3279, 3305, 3321, 3356, 3380, 3458, 3485, 3508, 3509, 3511, 3512, 3513, 3603, 3674, 3751, 3767, 3793, 3834, 3854, 3882, 3903, 3928, 3968, 3984, 4019, 4043, 4148, 4171, 4174, 4175, 4266, 4456, 4517, 4545, 4591, 4631, 4647, 4706, 4811, 4837, 4838, 5119, 5180, 5208, 5294, 5500, 5973, 6008, 6032, 6058, 6085, 6110, 6137, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6170, 6195, 6227, 6255, 6277, 6292, 6326, 6347, 6364, 6383, 6403, 6419, 6430, 6445, 6462, 6486, 6506, 6534, 6555, 6580, 6594, 6620, 6636, 6671, 6695, 6721, 6748, 6773, 6800, 6823, 6824, 6825, 6826, 6827, 6828, 6833, 6858, 6890, 6918, 6955, 6989, 7010, 7046, 7066, 7082, 7108, 7149, 7169, 7197, 7218, 7243, 7283, 7299, 7334, 7358, 7384, 7411, 7436, 7463, 7486, 7487, 7488, 7489, 7490, 7491, 7496, 7521, 7553, 7581, 7603, 7618, 7652, 7673, 7709, 7729, 7745, 7756, 7771, 7788, 7812, 7832, 7860, 7881, 7906, 7920, 7946, 7962, 7997, 8021, 8047, 8074, 8099, 8124, 8126, 8148, 8149, 8150, 8151, 8153, 8154, 8159, 8184, 8216, 8244, 8266, 8281, 8315, 8336, 8353, 8372, 8392, 8408, 8419, 8434, 8451, 8475, 8495, 8523, 8544, 8569, 8583, 8609, 8625, 8660, 8684, 8710, 8737, 8762, 8789, 8812, 8813, 8814, 8815, 8816, 8817, 8822, 8847, 8879, 8907, 8929, 8944, 8978, 8999, 9016, 9035, 9055, 9071, 9082, 9097, 9114, 9138, 9158, 9186, 9207, 9232, 9272, 9288, 9323, 9347, 9373, 9400, 9425, 9452, 9474, 9475, 9476, 9477, 9478, 9479, 9480, 9485, 9510, 9542, 9570, 9592, 9607, 9641, 9662, 9679, 9698, 9718, 9734, 9745, 9760, 9777, 9801, 9821, 9849, 9870, 9895, 9909, 9935, 9951, 10141, 10484), 2: (5742, 3753, 5079, 5304, 5306, 5307, 5308, 5309, 5312, 5315, 5316, 5317, 5319, 5321, 5322, 5324, 5326, 5331, 5332, 5334, 5335, 5336, 5338, 5340, 5341, 5342, 5343, 5344, 5347, 5348, 5350, 5351, 5352, 5354, 5359, 5361, 5363, 5364, 5365, 5366, 5368, 5374, 5375, 5376, 5378, 5381, 5388, 5391, 5398, 5403, 5404, 5405, 5407, 5418, 5419, 5420, 5421, 5424, 5427, 5428, 5429, 5431, 5436, 5440, 5441, 5444, 5445, 5446, 5452, 5453, 5454, 5456, 5463, 5465, 5466, 5467, 5470, 5472, 5473, 5476, 5477, 5478, 5479, 5480, 5482, 5483, 5484, 5486, 5490, 5494, 5505, 5508, 5511, 5512, 5514, 5534, 5536, 5541, 5542, 5543, 5545, 5555, 5557, 5558, 5559, 5566, 5567, 5569, 5572, 5573, 5574, 5576, 5581, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5611, 5612, 5613, 5615, 5618, 5620, 5621, 5622, 5625, 5627, 5628, 5632, 5633, 5634, 5635, 5636, 5639, 5640, 5641, 5643, 5646, 5652, 5654, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5665, 5666, 5667, 5669, 5670, 5671, 5673, 5674, 5675, 5676, 5678, 5682, 5683, 5687, 5688, 5690, 5695, 5705, 5706, 5707, 5709, 5717, 5718, 5719, 5722, 5723, 5725, 5726, 5727, 5729, 5730, 5731, 5732, 5733, 5734, 5736, 5737, 5738, 5739, 5740, 5741, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5753, 5754, 5755, 5757, 5758, 5759, 5760, 5761, 5762, 5770, 5771, 5779, 5780, 5781, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5793, 5802, 5804, 5807, 5808, 5809, 5811, 5812, 5821, 5822, 5825, 5826, 5827, 5832, 5834, 5835, 5836, 5837, 5839, 5841, 5842, 5846, 5847, 5848, 5849, 5850, 5852, 5853, 5854, 5856, 5858, 5859, 5860, 5861, 5865, 5868, 5869, 5870, 5872, 5873, 5879, 5880, 5881, 5886, 5895, 5900, 5901, 5902, 5904, 5909, 5915, 5920, 5934, 5939, 5940, 5941, 5943, 5950, 5954, 5955, 5956, 5958, 5959, 5960, 5961, 5963, 5965), 3: (6433, 677, 712, 736, 789, 844, 961, 1001, 1103, 1123, 1128, 1129, 1130, 1131, 1135, 1214, 1262, 1399, 1792, 1877, 2666, 2701, 2725, 2754, 2778, 2803, 2833, 2861, 2892, 2923, 2950, 2990, 3020, 3037, 3056, 3076, 3092, 3112, 3113, 3114, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3158, 3203, 3251, 3290, 3781, 4051, 4444, 4529, 5107, 5192, 5981, 6005, 6016, 6040, 6069, 6093, 6118, 6148, 6176, 6207, 6238, 6265, 6298, 6305, 6335, 6352, 6371, 6391, 6407, 6426, 6427, 6428, 6429, 6431, 6432, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6473, 6518, 6566, 6605, 6644, 6679, 6703, 6756, 6811, 6928, 6968, 6998, 7070, 7090, 7095, 7096, 7097, 7098, 7101, 7102, 7181, 7229, 7342, 7366, 7591, 7733, 7759, 7760, 7761, 7765, 7844, 7892, 7970, 8005, 8029, 8082, 8137, 8254, 8294, 8324, 8396, 8416, 8421, 8422, 8423, 8424, 8425, 8427, 8428, 8507, 8555, 8633, 8668, 8692, 8721, 8745, 8770, 8800, 8828, 8859, 8890, 8917, 8957, 8987, 9004, 9023, 9043, 9059, 9079, 9080, 9081, 9083, 9084, 9085, 9086, 9087, 9088, 9089, 9090, 9091, 9092, 9125, 9170, 9218, 9257, 9331, 9355, 9580, 9722, 9742, 9747, 9748, 9749, 9750, 9753, 9754, 9833, 9881, 10411), 4: (5989, 22, 529, 664, 685, 689, 721, 943, 1095, 1137, 1178, 1192, 1243, 1272, 1312, 1327, 1348, 1384, 1841, 1855, 1906, 1935, 2653, 2674, 2675, 2676, 2677, 2678, 2687, 2710, 2734, 2765, 2787, 2812, 2851, 2870, 2904, 2932, 2958, 2967, 3003, 3028, 3045, 3064, 3084, 3100, 3111, 3126, 3139, 3167, 3181, 3215, 3225, 3232, 3261, 3271, 3301, 3337, 3844, 4000, 4036, 4507, 4663, 5170, 5968, 5990, 5991, 5992, 5993, 6002, 6025, 6049, 6080, 6102, 6127, 6166, 6185, 6219, 6247, 6273, 6282, 6318, 6343, 6360, 6379, 6399, 6415, 6441, 6454, 6482, 6496, 6530, 6540, 6547, 6576, 6586, 6616, 6631, 6652, 6653, 6654, 6656, 6665, 6688, 6743, 6765, 6910, 7062, 7078, 7104, 7145, 7159, 7193, 7210, 7239, 7279, 7294, 7315, 7351, 7767, 7808, 7822, 7873, 7902, 7942, 7957, 7978, 7982, 8014, 8236, 8388, 8430, 8471, 8485, 8519, 8529, 8536, 8565, 8605, 8620, 8641, 8642, 8643, 8644, 8645, 8654, 8677, 8701, 8732, 8754, 8779, 8818, 8837, 8871, 8899, 8925, 8934, 8970, 8995, 9012, 9031, 9051, 9067, 9078, 9093, 9106, 9134, 9148, 9182, 9192, 9199, 9228, 9238, 9268, 9283, 9304, 9308, 9340, 9562, 9714, 9756, 9797, 9811, 9862, 9891, 9931, 9967), 5: (2963, 671, 706, 730, 758, 783, 954, 971, 974, 1100, 1116, 1142, 1317, 1393, 1637, 2660, 2665, 2679, 2690, 2695, 2700, 2719, 2724, 2747, 2753, 2768, 2772, 2777, 2797, 2802, 2826, 2832, 2857, 2860, 2885, 2916, 2922, 2939, 2942, 2943, 2949, 2953, 2959, 2960, 2961, 2964, 2965, 2982, 2989, 3013, 3019, 3033, 3051, 3055, 3070, 3075, 3087, 3089, 3090, 3091, 3101, 3102, 3103, 3105, 3106, 3107, 3108, 3109, 3110, 3129, 3131, 3132, 3133, 3134, 3151, 3172, 3173, 3196, 3202, 3220, 3221, 3244, 3250, 3266, 3283, 3304, 3306, 3307, 4289, 5975, 6010, 6034, 6062, 6087, 6112, 6141, 6172, 6200, 6231, 6258, 6274, 6275, 6276, 6278, 6279, 6280, 6297, 6328, 6348, 6366, 6385, 6404, 6420, 6446, 6466, 6487, 6511, 6535, 6559, 6581, 6621, 6673, 6697, 6750, 6921, 6938, 6941, 7067, 7083, 7109, 7360, 7604, 7746, 7772, 7964, 7999, 8023, 8051, 8247, 8264, 8267, 8393, 8409, 8435, 8500, 8570, 8627, 8662, 8686, 8714, 8739, 8764, 8793, 8824, 8852, 8883, 8910, 8926, 8927, 8928, 8930, 8931, 8932, 8949, 8980, 9000, 9018, 9037, 9056, 9072, 9098, 9118, 9139, 9163, 9187, 9211, 9233, 9273, 9349, 9593, 9735, 9761), 6: (3053, 673, 708, 732, 761, 785, 957, 1057, 1058, 1064, 1065, 1066, 1068, 1073, 1210, 1395, 1727, 2662, 2697, 2721, 2750, 2774, 2799, 2829, 2888, 2919, 2946, 2986, 3016, 3046, 3047, 3048, 3050, 3052, 3054, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3072, 3154, 3199, 3247, 3286, 3716, 4047, 4379, 5977, 6012, 6036, 6065, 6089, 6114, 6144, 6203, 6234, 6261, 6301, 6331, 6361, 6362, 6363, 6365, 6367, 6368, 6369, 6370, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6387, 6469, 6514, 6562, 6640, 6675, 6699, 6728, 6752, 6924, 7024, 7025, 7031, 7032, 7033, 7035, 7177, 7338, 7362, 7391, 7587, 7688, 7694, 7695, 7698, 7966, 8001, 8025, 8054, 8078, 8250, 8350, 8351, 8357, 8358, 8359, 8361, 8365, 8366, 8503, 8551, 8629, 8664, 8688, 8717, 8741, 8766, 8796, 8855, 8886, 8913, 8953, 8983, 9013, 9014, 9015, 9017, 9019, 9020, 9021, 9022, 9024, 9025, 9026, 9027, 9028, 9029, 9030, 9039, 9121, 9166, 9214, 9253, 9327, 9351, 9380, 9576, 9677, 9683, 9684, 9687), 7: (3034, 672, 707, 731, 760, 784, 839, 956, 996, 1041, 1046, 1047, 1049, 1054, 1394, 2661, 2696, 2720, 2749, 2773, 2798, 2820, 2828, 2858, 2887, 2918, 2945, 2983, 2985, 3014, 3015, 3029, 3030, 3031, 3035, 3036, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3071, 3153, 3198, 3246, 3285, 5976, 6011, 6035, 6064, 6088, 6113, 6143, 6173, 6202, 6233, 6260, 6300, 6330, 6344, 6345, 6346, 6350, 6351, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6386, 6468, 6513, 6561, 6600, 6674, 6698, 6727, 6751, 6806, 6923, 6963, 7008, 7012, 7013, 7016, 7337, 7361, 7586, 7671, 7676, 7679, 7965, 8000, 8024, 8053, 8077, 8132, 8249, 8289, 8333, 8334, 8339, 8340, 8342, 8346, 8347, 8502, 8628, 8663, 8687, 8716, 8740, 8765, 8795, 8825, 8854, 8885, 8912, 8952, 8982, 8996, 8997, 8998, 9001, 9002, 9003, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 9038, 9120, 9165, 9213, 9252, 9326, 9350, 9575, 9660, 9665, 9668), 8: (3073, 674, 709, 733, 786, 958, 1077, 1084, 1085, 1086, 1088, 1093, 1211, 1396, 1747, 1755, 2663, 2698, 2722, 2751, 2775, 2800, 2830, 2889, 2920, 2947, 2948, 2951, 2987, 3017, 3065, 3066, 3067, 3069, 3074, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3093, 3155, 3200, 3201, 3248, 3287, 4399, 5062, 5978, 6013, 6037, 6066, 6090, 6115, 6145, 6204, 6235, 6262, 6302, 6332, 6380, 6381, 6382, 6384, 6388, 6389, 6390, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6470, 6515, 6563, 6602, 6641, 6676, 6700, 6753, 6925, 7044, 7051, 7052, 7055, 7060, 7178, 7363, 7707, 7714, 7715, 7718, 7967, 8002, 8026, 8055, 8079, 8251, 8370, 8377, 8378, 8379, 8381, 8385, 8386, 8504, 8630, 8665, 8689, 8718, 8742, 8767, 8797, 8856, 8887, 8914, 8954, 8984, 9032, 9033, 9034, 9036, 9040, 9041, 9042, 9044, 9045, 9046, 9047, 9048, 9049, 9050, 9122, 9167, 9215, 9352, 9577, 9696, 9703, 9704, 9707, 9712), 9: (2714, 720, 725, 1196, 1388, 2652, 2655, 2664, 2667, 2668, 2669, 2670, 2672, 2683, 2709, 2711, 2712, 2713, 2715, 2716, 2718, 2723, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2738, 2739, 2791, 2793, 2816, 2874, 2908, 2931, 2936, 2971, 2994, 3007, 3143, 3175, 3180, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3192, 3193, 3194, 3195, 3197, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3229, 3236, 3257, 3268, 3275, 3311, 3313, 4040, 6024, 6026, 6027, 6028, 6029, 6030, 6031, 6038, 6039, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6053, 6106, 6131, 6189, 6223, 6251, 6286, 6322, 6458, 6500, 6551, 6687, 6692, 7163, 7355, 7826, 8013, 8018, 8030, 8034, 8035, 8489, 8676, 8678, 8679, 8680, 8681, 8682, 8690, 8691, 8693, 8694, 8695, 8696, 8697, 8698, 8699, 8705, 8758, 8783, 8841, 8903, 8938, 9152, 9203, 9344, 9815), 10: (827, 164, 716, 722, 723, 724, 726, 727, 729, 734, 735, 737, 738, 739, 740, 741, 742, 743, 749, 802, 822, 823, 824, 825, 826, 828, 829, 830, 831, 832, 834, 835, 836, 837, 838, 840, 841, 842, 843, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 885, 889, 919, 947, 982, 986, 1005, 1018, 1035, 1154, 1195, 1200, 1203, 1206, 1208, 1209, 1212, 1215, 1218, 1220, 1240, 1247, 1286, 1490, 2153, 3479, 4142, 4805, 5468, 6153, 6504, 6689, 6690, 6691, 6693, 6694, 6696, 6701, 6702, 6704, 6705, 6706, 6707, 6709, 6716, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6801, 6803, 6805, 6808, 6809, 6810, 6812, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6886, 6914, 6949, 6953, 6985, 7167, 7457, 7830, 8120, 8493, 9156, 9446, 9819, 10109), 11: (3298, 773, 1223, 1281, 1284, 1299, 1309, 1310, 1947, 1949, 1972, 2762, 2841, 2900, 2999, 3270, 3272, 3273, 3274, 3276, 3277, 3278, 3280, 3282, 3284, 3288, 3289, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3299, 3300, 3961, 4624, 5287, 6077, 6156, 6215, 6314, 6527, 6585, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6596, 6597, 6598, 6599, 6603, 6604, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6740, 7248, 7251, 7253, 7266, 7276, 7403, 7911, 7914, 7916, 7929, 7932, 7938, 7939, 8066, 8516, 8574, 8577, 8579, 8592, 8595, 8600, 8601, 8602, 8603, 8808, 8867, 8966, 9237, 9239, 9240, 9241, 9242, 9243, 9244, 9245, 9247, 9248, 9249, 9250, 9251, 9254, 9255, 9256, 9258, 9259, 9260, 9261, 9262, 9263, 9264, 9265, 9266, 9267, 9392, 9900, 9903, 9905, 9918, 9921, 9928, 9929), 12: (2544, 1989, 1991, 1992, 1993, 2001, 2002, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2016, 2020, 2023, 2027, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2075, 2076, 2105, 2123, 2128, 2130, 2143, 2144, 2175, 2245, 2268, 2273, 2279, 2291, 2308, 2331, 2344, 2427, 2432, 2433, 2443, 2445, 2470, 2471, 2472, 2475, 2480, 2497, 2512, 2513, 2514, 2517, 2518, 2519, 2520, 2521, 2522, 2524, 2525, 2526, 2527, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2554, 2558, 2559, 2563, 2564, 2566, 2568, 2573, 2593, 2594, 2604, 2605, 2607, 2612, 2635, 2646, 2647, 2648, 2650), 13: (2914, 756, 917, 925, 926, 932, 935, 940, 1588, 2702, 2745, 2824, 2834, 2883, 2903, 2905, 2906, 2907, 2909, 2910, 2911, 2913, 2915, 2917, 2921, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2956, 2980, 3127, 3149, 3242, 3281, 3308, 4240, 6060, 6139, 6198, 6218, 6220, 6221, 6222, 6224, 6225, 6226, 6228, 6229, 6230, 6232, 6236, 6237, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6295, 6509, 6557, 6884, 6892, 6893, 6899, 6902, 7547, 7549, 7555, 7565, 8049, 8207, 8210, 8211, 8212, 8218, 8219, 8225, 8228, 8232, 8233, 8498, 8791, 8850, 8870, 8872, 8873, 8874, 8875, 8876, 8877, 8878, 8880, 8881, 8882, 8884, 8888, 8889, 8891, 8892, 8893, 8894, 8895, 8896, 8897, 8947, 9209, 9375, 9536, 9537, 9538, 9544, 9545, 9551, 9554, 9559), 14: (6171, 867, 873, 874, 952, 1392, 1530, 1866, 2659, 2694, 2744, 2771, 2796, 2823, 2852, 2853, 2854, 2856, 2859, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2881, 2941, 2978, 3012, 3241, 3519, 4182, 4518, 5974, 6009, 6033, 6059, 6086, 6111, 6135, 6138, 6167, 6168, 6169, 6174, 6175, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6196, 6256, 6257, 6293, 6327, 6329, 6448, 6463, 6507, 6556, 6595, 6637, 6749, 6834, 6837, 6838, 6840, 6841, 6845, 6919, 7497, 7503, 7833, 7963, 7998, 8022, 8160, 8163, 8164, 8166, 8167, 8170, 8171, 8245, 8496, 8545, 8626, 8661, 8738, 8763, 8790, 8819, 8820, 8821, 8823, 8826, 8827, 8829, 8830, 8831, 8832, 8833, 8834, 8835, 8848, 8908, 8945, 8979, 9208, 9348, 9486, 9492, 9571, 9822), 15: (972, 309, 668, 691, 695, 753, 780, 805, 808, 865, 868, 890, 896, 922, 927, 950, 970, 973, 975, 976, 987, 993, 1021, 1024, 1059, 1062, 1078, 1081, 1098, 1102, 1114, 1117, 1118, 1140, 1157, 1162, 1181, 1183, 1201, 1207, 1229, 1231, 1239, 1250, 1255, 1275, 1277, 1289, 1294, 1315, 1635, 2298, 3624, 4287, 4950, 6418, 6635, 6638, 6658, 6662, 6720, 6725, 6747, 6772, 6775, 6799, 6804, 6832, 6835, 6857, 6863, 6889, 6894, 6917, 6937, 6939, 6940, 6942, 6943, 6954, 6960, 6988, 6991, 7026, 7029, 7045, 7048, 7065, 7081, 7107, 7124, 7129, 7148, 7150, 7196, 7198, 7206, 7217, 7222, 7242, 7244, 7256, 7282, 7284, 7602, 7744, 8265, 8407, 9070, 9591, 9733, 10254), 16: (10494, 549, 1875, 3864, 4527, 5190, 6516, 7842, 8505, 9831, 9945, 9956, 9957, 9958, 9960, 9976, 9992, 10002, 10004, 10005, 10006, 10007, 10008, 10009, 10015, 10016, 10017, 10019, 10021, 10022, 10024, 10045, 10069, 10094, 10124, 10152, 10183, 10201, 10214, 10224, 10227, 10229, 10235, 10241, 10246, 10247, 10249, 10264, 10281, 10287, 10300, 10311, 10347, 10366, 10367, 10372, 10373, 10377, 10378, 10380, 10382, 10383, 10384, 10385, 10386, 10387, 10388, 10389, 10390, 10391, 10392, 10401, 10421, 10426, 10427, 10428, 10449, 10473, 10474, 10475, 10476, 10478, 10480, 10481, 10482, 10483, 10485, 10486, 10487, 10488, 10490, 10491, 10492, 10493, 10495, 10496, 10497, 10498, 10499, 10500, 10501, 10502, 10503, 10504, 10505, 10506, 10507, 10511, 10514, 10522, 10542, 10581), 17: (9153, 534, 1197, 1412, 1860, 2523, 2733, 2735, 2736, 2737, 2741, 2746, 2752, 2755, 2757, 2758, 2759, 2760, 2761, 2763, 3849, 4064, 4512, 5175, 5838, 6048, 6050, 6051, 6052, 6054, 6055, 6056, 6061, 6067, 6068, 6070, 6072, 6073, 6074, 6075, 6076, 6078, 6079, 6132, 6501, 6717, 7379, 7827, 8042, 8043, 8490, 8683, 8685, 8700, 8702, 8703, 8704, 8706, 8707, 8708, 8709, 8711, 8712, 8713, 8715, 8719, 8720, 8722, 8723, 8724, 8725, 8726, 8727, 8728, 8729, 8730, 8731, 8784, 8842, 8939, 8961, 9110, 9147, 9149, 9150, 9154, 9155, 9157, 9159, 9160, 9161, 9162, 9164, 9168, 9169, 9171, 9172, 9173, 9174, 9175, 9176, 9177, 9178, 9179, 9180, 9181, 9368, 9816, 10479), 18: (7162, 532, 697, 698, 699, 700, 701, 702, 703, 705, 710, 711, 713, 714, 715, 717, 718, 719, 748, 801, 884, 918, 946, 981, 1017, 1153, 1191, 1193, 1198, 1213, 1217, 1246, 1285, 1858, 3847, 4510, 5173, 6018, 6499, 6664, 6666, 6667, 6668, 6669, 6670, 6672, 6677, 6678, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6715, 6768, 6851, 6885, 6913, 6948, 6972, 6984, 7120, 7158, 7160, 7161, 7164, 7165, 7166, 7168, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7179, 7180, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7213, 7214, 7252, 7354, 7825, 8017, 8488, 9151, 9814, 10477), 19: (1185, 522, 678, 679, 766, 768, 790, 791, 815, 816, 904, 906, 936, 953, 962, 963, 1002, 1004, 1032, 1033, 1089, 1104, 1105, 1138, 1139, 1143, 1144, 1145, 1146, 1147, 1148, 1170, 1172, 1179, 1180, 1184, 1186, 1187, 1188, 1189, 1190, 1263, 1265, 1302, 1304, 1319, 1320, 1848, 3174, 3837, 4500, 5163, 6489, 6645, 6646, 6733, 6735, 6757, 6758, 6782, 6783, 6871, 6873, 6903, 6929, 6930, 6969, 6971, 6999, 7000, 7056, 7071, 7072, 7105, 7106, 7110, 7111, 7112, 7113, 7114, 7115, 7137, 7146, 7147, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7230, 7232, 7269, 7286, 7287, 7815, 8478, 9141, 9804, 10467), 20: (3867, 74, 552, 1400, 1878, 3315, 3327, 3328, 3330, 3365, 3372, 3374, 3375, 3376, 3377, 3378, 3379, 3384, 3385, 3386, 3389, 3390, 3391, 3392, 3394, 3442, 3454, 3467, 3497, 3525, 3556, 3566, 3571, 3577, 3587, 3590, 3594, 3599, 3605, 3614, 3617, 3634, 3654, 3657, 3684, 3701, 3720, 3740, 3756, 3759, 3782, 3789, 3790, 3791, 3792, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3822, 3843, 3845, 3846, 3848, 3850, 3851, 3852, 3853, 3855, 3856, 3857, 3858, 3860, 3861, 3862, 3863, 3865, 3866, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3892, 3899, 3915, 3954, 3971, 4715, 5193, 6519, 8508, 9834), 21: (5077, 41, 118, 143, 170, 193, 194, 195, 260, 288, 310, 325, 359, 380, 416, 436, 452, 567, 1521, 1586, 1636, 1742, 2425, 3433, 3510, 3518, 3575, 3625, 3640, 3695, 3712, 3731, 3778, 3810, 4414, 4682, 4688, 4732, 4759, 4784, 4820, 4833, 4834, 4835, 4836, 4839, 4844, 4848, 4869, 4901, 4929, 4937, 4951, 4966, 4977, 5000, 5007, 5021, 5024, 5038, 5057, 5063, 5076, 5082, 5093, 5104, 5136, 5160, 5229, 5254, 5268, 5310, 5345, 5422, 5474, 5497, 5498, 5499, 5564, 5592, 5614, 5629, 5663, 5684, 5720, 5756, 5782, 5843, 5871, 5957, 10140, 10233, 10381), 22: (3160, 1817, 1834, 2894, 2992, 3138, 3140, 3141, 3142, 3144, 3145, 3146, 3148, 3150, 3152, 3156, 3157, 3159, 3161, 3162, 3163, 3164, 3165, 3166, 3253, 4486, 5149, 6150, 6209, 6307, 6453, 6455, 6456, 6457, 6459, 6460, 6464, 6465, 6467, 6471, 6472, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6520, 6568, 7782, 7784, 7801, 8060, 8442, 8445, 8447, 8460, 8463, 8464, 8468, 8509, 8802, 8861, 8959, 9105, 9107, 9108, 9109, 9111, 9112, 9115, 9117, 9123, 9124, 9126, 9127, 9128, 9129, 9130, 9131, 9132, 9133, 9220, 9386, 9771, 9773, 9790), 23: (1048, 385, 765, 809, 814, 869, 872, 898, 903, 929, 934, 1026, 1031, 1039, 1040, 1042, 1044, 1050, 1051, 1052, 1053, 1055, 1063, 1067, 1082, 1087, 1122, 1124, 1125, 1126, 1127, 1132, 1133, 1134, 1136, 1164, 1169, 1257, 1296, 1301, 1711, 2374, 3700, 4363, 5026, 5689, 6639, 6732, 6776, 6781, 6836, 6839, 6870, 6896, 6901, 6993, 7007, 7009, 7011, 7014, 7015, 7017, 7018, 7020, 7021, 7022, 7030, 7034, 7049, 7054, 7091, 7092, 7093, 7094, 7099, 7100, 7103, 7224, 7268, 7678, 8341, 9667, 10330, 11069, 11076, 11079, 11207), 24: (543, 12, 15, 57, 59, 61, 62, 63, 64, 70, 71, 72, 79, 94, 173, 232, 256, 263, 279, 280, 281, 282, 283, 284, 286, 287, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 319, 329, 342, 438, 444, 481, 483, 498, 528, 530, 531, 533, 535, 536, 537, 538, 540, 541, 542, 545, 547, 548, 550, 551, 553, 554, 555, 556, 557, 558, 559, 561, 562, 577, 591, 1610, 1869, 4521, 5184, 6510, 7836, 8499, 9825), 25: (995, 332, 759, 897, 928, 955, 977, 979, 980, 983, 984, 985, 990, 991, 992, 994, 998, 999, 1000, 1003, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1015, 1016, 1019, 1020, 1025, 1028, 1029, 1030, 1036, 1037, 1038, 1163, 1256, 1295, 1658, 2321, 2984, 3647, 4310, 4973, 6299, 6726, 6922, 6944, 6946, 6947, 6951, 6952, 6959, 6961, 6962, 6965, 6966, 6970, 6974, 6975, 6976, 6978, 6979, 6980, 6982, 6983, 6986, 6987, 6992, 6995, 6996, 7002, 7003, 7004, 7223, 7625, 8288, 8951, 9614, 10277), 26: (1303, 640, 767, 905, 911, 1149, 1151, 1152, 1155, 1156, 1158, 1161, 1167, 1168, 1171, 1173, 1174, 1175, 1176, 1177, 1216, 1264, 1283, 1287, 1288, 1290, 1293, 1300, 1305, 1306, 1307, 1308, 1311, 1966, 2629, 3955, 4618, 5281, 5944, 6734, 6813, 6872, 6878, 6977, 7116, 7118, 7119, 7121, 7122, 7123, 7128, 7130, 7134, 7138, 7139, 7141, 7142, 7143, 7144, 7231, 7250, 7254, 7255, 7258, 7259, 7260, 7261, 7262, 7264, 7265, 7270, 7271, 7273, 7274, 7275, 7277, 7278, 7933, 8596, 9922, 10585, 11248), 27: (1061, 398, 670, 755, 782, 807, 810, 863, 864, 870, 871, 875, 876, 877, 878, 879, 892, 899, 924, 930, 989, 997, 1023, 1027, 1060, 1069, 1070, 1071, 1072, 1074, 1080, 1083, 1159, 1165, 1252, 1258, 1291, 1297, 1724, 2387, 3713, 4376, 5039, 5702, 6722, 6774, 6777, 6807, 6830, 6831, 6842, 6843, 6844, 6846, 6859, 6866, 6891, 6897, 6956, 6964, 6990, 6994, 7027, 7028, 7036, 7037, 7038, 7039, 7040, 7041, 7047, 7050, 7126, 7132, 7219, 7225, 7691, 8354, 9680, 10343), 28: (11152, 381, 544, 1707, 1870, 3696, 3859, 4522, 5022, 5185, 7674, 7837, 8337, 9826, 10326, 10489, 10613, 10616, 10665, 10667, 10671, 10672, 10675, 10677, 10678, 10680, 10684, 10687, 10688, 10703, 10709, 10728, 10737, 10753, 10758, 10813, 10841, 10864, 10872, 10882, 10914, 10916, 10918, 10920, 10921, 11007, 11015, 11026, 11035, 11058, 11064, 11072, 11107, 11113, 11128, 11132, 11137, 11139, 11142, 11144, 11149, 11150, 11155, 11161, 11166, 11167, 11168, 11170, 11176, 11178, 11184, 11200, 11206, 11222, 11239, 11245, 11262), 29: (3766, 451, 1777, 2440, 3320, 3323, 3340, 3343, 3347, 3355, 3358, 3363, 3432, 3435, 3440, 3457, 3465, 3484, 3489, 3495, 3517, 3520, 3523, 3542, 3554, 3574, 3579, 3585, 3602, 3606, 3612, 3622, 3626, 3628, 3639, 3645, 3652, 3673, 3676, 3682, 3711, 3714, 3718, 3730, 3733, 3738, 3750, 3752, 3754, 3763, 3764, 3765, 3768, 3769, 3771, 3773, 3777, 3779, 3780, 3809, 3820, 3833, 3835, 3836, 3881, 3883, 3891, 3902, 3913, 3927, 3967, 3969, 3970, 4429, 4928, 5092, 10396), 30: (7085, 455, 676, 680, 788, 792, 937, 960, 964, 1006, 1034, 1090, 1106, 1112, 1119, 1219, 1227, 1232, 1233, 1234, 1235, 1318, 1321, 1781, 3770, 4433, 5096, 6422, 6643, 6647, 6731, 6736, 6755, 6759, 6780, 6784, 6869, 6874, 6900, 6904, 6909, 6927, 6931, 6967, 6973, 6997, 7001, 7053, 7057, 7069, 7073, 7079, 7080, 7084, 7086, 7087, 7088, 7135, 7140, 7194, 7195, 7199, 7200, 7201, 7202, 7228, 7233, 7267, 7272, 7285, 7288, 7748, 8411, 9074, 9737, 10400), 31: (2444, 1994, 1997, 2032, 2037, 2041, 2084, 2090, 2095, 2109, 2114, 2118, 2134, 2139, 2169, 2174, 2197, 2201, 2228, 2233, 2259, 2263, 2280, 2286, 2290, 2295, 2296, 2297, 2300, 2301, 2302, 2326, 2332, 2356, 2360, 2373, 2377, 2392, 2396, 2412, 2416, 2426, 2428, 2437, 2438, 2439, 2441, 2442, 2447, 2454, 2458, 2468, 2469, 2488, 2494, 2499, 2509, 2510, 2552, 2553, 2555, 2556, 2557, 2560, 2561, 2581, 2587, 2592, 2603, 2620, 2626, 2631, 2643, 2644), 32: (3948, 633, 1515, 2622, 3324, 3359, 3436, 3461, 3491, 3504, 3550, 3563, 3608, 3648, 3662, 3678, 3692, 3693, 3694, 3698, 3699, 3702, 3703, 3705, 3706, 3707, 3715, 3734, 3816, 3909, 3934, 3935, 3936, 3937, 3939, 3940, 3941, 3943, 3944, 3945, 3946, 3947, 3949, 3950, 3951, 3952, 3953, 3956, 3957, 3959, 3960, 3962, 3963, 4611, 4830, 7926, 8589, 9915, 10955, 11228, 11230, 11232, 11233, 11234, 11238, 11242, 11243, 11246, 11247, 11249, 11255), 33: (1715, 389, 546, 1335, 1344, 1403, 1423, 1456, 1472, 1481, 1512, 1539, 1561, 1571, 1592, 1601, 1628, 1638, 1670, 1698, 1704, 1705, 1713, 1716, 1718, 1726, 1734, 1745, 1754, 1770, 1782, 1796, 1809, 1827, 1837, 1851, 1872, 1883, 1896, 1901, 1902, 1903, 1904, 1920, 1930, 1941, 1959, 1969, 1985, 2378, 3704, 4367, 5030, 5187, 7682, 8345, 9671, 10626, 10715, 10738, 10763, 10883, 11016, 11036, 11078, 11133, 11183, 11186, 11223, 11267), 34: (7064, 434, 666, 675, 693, 694, 696, 747, 763, 778, 787, 800, 812, 883, 945, 959, 1096, 1097, 1101, 1107, 1108, 1109, 1110, 1113, 1194, 1228, 1238, 1245, 1260, 1274, 1314, 1760, 3086, 3749, 4412, 5075, 6401, 6633, 6642, 6660, 6661, 6663, 6714, 6730, 6745, 6754, 6767, 6779, 6850, 6868, 6912, 6920, 6926, 7063, 7068, 7074, 7075, 7076, 7077, 7205, 7212, 7227, 7241, 7281, 7727, 8390, 9053, 9716, 10379), 35: (9953, 8, 1334, 1894, 3986, 4649, 7301, 9290, 9946, 9947, 9948, 9949, 9950, 9952, 9955, 9959, 9961, 9962, 9963, 9964, 9965, 9966, 9979, 9988, 9997, 10026, 10040, 10065, 10074, 10079, 10090, 10104, 10119, 10150, 10162, 10178, 10209, 10219, 10236, 10251, 10252, 10253, 10256, 10257, 10258, 10259, 10275, 10295, 10306, 10316, 10344, 10363, 10398, 10399, 10409, 10424, 10431, 10444, 10465, 10509, 10513, 10524, 10537, 10559, 10563, 10576, 10599, 10600, 10603), 36: (5188, 10, 69, 401, 407, 408, 410, 1336, 1733, 1873, 3325, 3722, 3723, 3725, 4525, 4651, 4686, 4739, 4763, 4788, 4818, 4877, 4908, 4935, 4975, 4983, 5005, 5035, 5036, 5037, 5040, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5061, 5143, 5169, 5171, 5172, 5176, 5177, 5178, 5179, 5182, 5183, 5186, 5189, 5191, 5194, 5195, 5197, 5198, 5199, 5200, 5202, 5203, 5236, 7840, 9829, 10014), 37: (10325, 2369, 4358, 9954, 9986, 10010, 10036, 10042, 10063, 10066, 10088, 10091, 10115, 10137, 10138, 10139, 10142, 10143, 10148, 10173, 10180, 10205, 10211, 10255, 10270, 10278, 10304, 10323, 10324, 10332, 10333, 10334, 10335, 10336, 10337, 10342, 10345, 10361, 10364, 10397, 10408, 10423, 10440, 10446, 10464, 10512, 10533, 10539, 10558, 10578, 10598, 10699, 10751, 10802, 10805, 10806, 10836, 10868, 11005, 11024, 11071, 11103, 11127, 11175, 11196, 11221), 38: (4699, 1, 58, 563, 573, 1889, 1899, 3316, 3373, 3878, 3888, 4642, 4664, 4665, 4666, 4667, 4676, 4698, 4700, 4701, 4702, 4703, 4704, 4705, 4710, 4711, 4714, 4716, 4717, 4718, 4719, 4720, 4721, 4727, 4754, 4776, 4780, 4801, 4859, 4893, 4897, 4921, 4925, 4947, 4956, 4960, 4992, 5034, 5053, 5073, 5089, 5100, 5115, 5128, 5132, 5156, 5174, 5196, 5204, 5214, 5221, 5225, 5250, 5290, 10003), 39: (5277, 347, 620, 624, 626, 630, 631, 635, 636, 637, 639, 1946, 1950, 1952, 1956, 1957, 1961, 1962, 1963, 1965, 2625, 4614, 4653, 4712, 4751, 4765, 4790, 4879, 4889, 4988, 5074, 5080, 5083, 5084, 5085, 5086, 5087, 5088, 5145, 5238, 5259, 5260, 5261, 5262, 5263, 5265, 5266, 5267, 5270, 5271, 5272, 5273, 5275, 5276, 5278, 5279, 5280, 5282, 5283, 5285, 5286, 5288, 5289), 40: (7006, 376, 686, 687, 688, 776, 798, 855, 862, 915, 969, 978, 1014, 1056, 1075, 1111, 1150, 1226, 1236, 1282, 1702, 2365, 3691, 4354, 5017, 5680, 6655, 6712, 6822, 6829, 6848, 6865, 6882, 6936, 6945, 6981, 7019, 7023, 7042, 7089, 7117, 7131, 7203, 7249, 7263, 7669, 8332, 9658, 10321, 10634, 10643, 10721, 10800, 10807, 10888, 10923, 11040, 11082, 11123, 11188, 11217, 11257), 41: (6544, 2782, 2807, 2954, 3096, 3177, 3222, 3227, 3230, 3256, 3267, 4535, 5967, 5985, 6097, 6122, 6212, 6269, 6311, 6411, 6423, 6450, 6492, 6495, 6521, 6522, 6524, 6536, 6537, 6542, 6545, 6571, 6582, 6626, 6628, 7370, 7850, 7870, 7974, 8033, 8481, 8513, 8533, 8560, 8571, 8615, 8637, 8774, 8921, 9063, 9102, 9144, 9189, 9196, 9197, 9223, 9234, 9278, 9359, 9839, 9859), 42: (931, 268, 762, 811, 894, 900, 914, 916, 920, 921, 933, 938, 939, 941, 1076, 1091, 1092, 1094, 1160, 1166, 1205, 1253, 1259, 1292, 1298, 1594, 2257, 3583, 4246, 4909, 6723, 6729, 6778, 6802, 6867, 6881, 6883, 6887, 6888, 6895, 6898, 6905, 6906, 6907, 6908, 6958, 7043, 7058, 7059, 7061, 7127, 7133, 7220, 7226, 7561, 8224, 9550, 10213, 10876), 43: (6057, 90, 1416, 2079, 2657, 2680, 2684, 2692, 2742, 2794, 3170, 3218, 3239, 3264, 3405, 4068, 4731, 5972, 5995, 5999, 6007, 6063, 6071, 6084, 6109, 6136, 6190, 6194, 6254, 6291, 6325, 6402, 6444, 6461, 6485, 6505, 6533, 6543, 6554, 6579, 6619, 7383, 8020, 8046, 8624, 8647, 8651, 8659, 8906, 9054, 9096, 9137, 9185, 9206, 9231, 9271, 9372, 10035, 10698), 44: (4847, 204, 206, 2195, 4184, 4648, 4650, 4683, 4733, 4760, 4762, 4785, 4787, 4812, 4817, 4840, 4841, 4842, 4843, 4845, 4846, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4870, 4876, 4902, 4930, 4967, 4974, 5001, 5004, 5019, 5020, 5025, 5028, 5029, 5031, 5032, 5033, 5041, 5058, 5060, 5137, 5142, 5181, 5230, 5235, 5269, 5274, 5510, 7499, 9488), 45: (2991, 339, 1665, 2779, 2804, 2893, 2966, 2968, 2969, 2970, 2973, 2974, 2976, 2979, 2981, 2988, 2993, 2995, 2996, 2997, 2998, 3000, 3001, 3128, 3135, 3136, 3137, 3252, 4317, 4980, 6281, 6283, 6284, 6285, 6288, 6290, 6296, 6303, 6304, 6306, 6309, 6315, 7632, 8295, 8933, 8935, 8936, 8937, 8940, 8948, 8950, 8955, 8956, 8958, 8967, 9621, 10284), 46: (3595, 252, 412, 432, 474, 1351, 1578, 1606, 1738, 1758, 1800, 2269, 3338, 3339, 3341, 3350, 3428, 3450, 3475, 3488, 3514, 3533, 3567, 3578, 3596, 3597, 3598, 3601, 3604, 3607, 3610, 3611, 3615, 3616, 3618, 3619, 3620, 3621, 3630, 3644, 3666, 3708, 3727, 3747, 3774, 3802, 3813, 3830, 3895, 3906, 3924, 3964, 4258, 5778, 7573, 10225), 47: (2271, 1608, 2013, 2019, 2021, 2022, 2026, 2073, 2104, 2126, 2151, 2190, 2209, 2221, 2243, 2252, 2270, 2272, 2274, 2275, 2276, 2278, 2281, 2282, 2284, 2285, 2287, 2288, 2289, 2292, 2293, 2294, 2306, 2318, 2342, 2367, 2384, 2403, 2423, 2430, 2431, 2435, 2450, 2478, 2487, 2506, 2571, 2580, 2600, 2610, 2619, 2640, 4260, 7575, 8238, 9564), 48: (8524, 568, 4546, 5209, 7861, 7969, 7973, 8008, 8032, 8062, 8076, 8085, 8101, 8110, 8130, 8141, 8161, 8168, 8189, 8200, 8220, 8257, 8262, 8263, 8268, 8269, 8286, 8299, 8317, 8327, 8344, 8355, 8363, 8374, 8383, 8420, 8438, 8455, 8466, 8476, 8480, 8512, 8520, 8522, 8525, 8526, 8527, 8528, 8548, 8559, 8587, 8598, 8610, 8614, 9850), 49: (492, 39, 113, 114, 115, 116, 122, 123, 125, 127, 128, 129, 130, 131, 132, 133, 140, 166, 183, 224, 242, 257, 321, 340, 356, 486, 487, 488, 489, 490, 493, 497, 500, 502, 503, 505, 509, 510, 511, 512, 513, 514, 585, 601, 1818, 2481, 4470, 5129, 5131, 5133, 5146, 5796, 8448, 9774, 10437), 50: (3819, 504, 508, 1820, 1824, 1829, 1830, 1831, 2493, 3362, 3439, 3464, 3494, 3498, 3522, 3553, 3557, 3584, 3651, 3655, 3681, 3717, 3737, 3748, 3757, 3760, 3761, 3762, 3801, 3803, 3804, 3805, 3807, 3808, 3811, 3812, 3814, 3815, 3817, 3818, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3912, 3916, 4482, 5135, 5139, 7797, 9786), 51: (10118, 1499, 2162, 4151, 7466, 8129, 9455, 9983, 10105, 10106, 10107, 10108, 10111, 10112, 10113, 10114, 10116, 10117, 10120, 10122, 10123, 10125, 10126, 10127, 10128, 10130, 10131, 10132, 10133, 10134, 10135, 10136, 10171, 10177, 10208, 10226, 10228, 10231, 10232, 10234, 10240, 10242, 10243, 10244, 10248, 10250, 10268, 10274, 10276, 10307, 10314, 10443, 10536, 10575), 52: (5371, 67, 2056, 3382, 3387, 3627, 4045, 4708, 4713, 5356, 5362, 5367, 5369, 5370, 5372, 5373, 5377, 5379, 5380, 5382, 5383, 5384, 5390, 5394, 5399, 5433, 5443, 5449, 5509, 5526, 5537, 5560, 5568, 5610, 5616, 5617, 5623, 5685, 5703, 5752, 5768, 5795, 5803, 5824, 5844, 5851, 5888, 5896, 5918, 5927, 5935, 10012), 53: (8974, 355, 1681, 2748, 2827, 2944, 3002, 3005, 3006, 3009, 3018, 3021, 3022, 3023, 3024, 3025, 3026, 3245, 3670, 4333, 4996, 6317, 6321, 6336, 6338, 6339, 6512, 7648, 8311, 8501, 8794, 8853, 8911, 8969, 8971, 8972, 8973, 8975, 8976, 8977, 8981, 8985, 8986, 8988, 8989, 8990, 8991, 8992, 8993, 9119, 9212, 9637), 54: (379, 5, 9, 25, 28, 32, 40, 44, 117, 121, 142, 146, 169, 176, 227, 235, 259, 324, 333, 358, 363, 377, 378, 383, 384, 386, 387, 388, 390, 391, 392, 396, 400, 415, 419, 435, 462, 477, 494, 518, 566, 576, 587, 594, 612, 652, 2368, 4357, 7672, 8335, 9661), 55: (4046, 4022, 4035, 4037, 4038, 4039, 4041, 4042, 4048, 4049, 4050, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4099, 4117, 4124, 4154, 4213, 4234, 4244, 4262, 4271, 4297, 4311, 4320, 4341, 4355, 4356, 4359, 4361, 4362, 4364, 4366, 4368, 4369, 4370, 4378, 4397, 4469, 4511, 4515, 4523, 4524, 4562, 4572, 4601), 56: (7680, 2376, 4365, 7309, 7344, 7368, 7414, 7421, 7439, 7446, 7469, 7477, 7504, 7528, 7536, 7566, 7593, 7626, 7634, 7656, 7663, 7670, 7677, 7681, 7683, 7684, 7685, 7693, 7699, 7712, 7719, 7735, 7802, 7809, 7810, 7811, 7813, 7814, 7816, 7820, 7839, 7847, 7887, 7895, 7934, 7950, 8343, 9669, 11125), 57: (6443, 476, 1802, 2465, 2682, 2689, 2767, 2934, 3169, 3303, 4454, 5117, 5970, 5982, 5997, 5998, 6000, 6004, 6017, 6082, 6094, 6104, 6119, 6129, 6149, 6187, 6208, 6249, 6266, 6320, 6405, 6408, 6417, 6442, 6451, 6452, 6484, 6498, 6549, 6567, 6578, 6618, 6623, 7769, 8432, 8650, 8901, 9095, 9758), 58: (7211, 581, 665, 690, 692, 799, 882, 1221, 1242, 1244, 1249, 1261, 1266, 1268, 1269, 1270, 1273, 1313, 1907, 2570, 3233, 3896, 4559, 5222, 5885, 6548, 6632, 6657, 6659, 6766, 6849, 6876, 7204, 7209, 7216, 7234, 7235, 7236, 7237, 7238, 7240, 7280, 7874, 8537, 9200, 9863, 10526, 11189), 59: (5357, 53, 1379, 2042, 2705, 3368, 4031, 4694, 5339, 5346, 5349, 5353, 5355, 5358, 5360, 5389, 5411, 5434, 5442, 5459, 5489, 5501, 5517, 5525, 5549, 5579, 5648, 5693, 5712, 5774, 5794, 5815, 5829, 5874, 5877, 5878, 5882, 5887, 5908, 5919, 5926, 5947, 6020, 7346, 8009, 8672, 9335, 9998), 60: (4953, 312, 456, 4290, 4659, 4684, 4736, 4761, 4771, 4786, 4796, 4815, 4827, 4874, 4886, 4905, 4916, 4932, 4943, 4948, 4949, 4952, 4954, 4971, 4985, 5002, 5013, 5059, 5069, 5094, 5097, 5120, 5124, 5140, 5152, 5161, 5166, 5211, 5218, 5219, 5233, 5245, 5255, 5256, 5295, 5300, 7605, 9594), 61: (948, 285, 667, 777, 779, 793, 794, 795, 796, 803, 887, 895, 942, 944, 949, 965, 966, 967, 968, 1248, 1254, 1611, 2937, 3600, 4263, 4926, 6252, 6634, 6744, 6746, 6760, 6761, 6762, 6763, 6770, 6854, 6911, 6915, 6916, 6934, 7215, 7221, 7578, 8241, 8904, 9567, 10230), 62: (2817, 165, 1491, 2154, 2740, 2756, 2764, 2811, 2813, 2814, 2815, 2818, 2819, 2821, 2825, 2831, 2835, 2836, 2837, 2838, 2839, 2840, 2842, 2843, 2875, 2878, 2975, 3480, 4143, 4806, 5469, 7458, 8121, 8778, 8781, 8782, 8787, 8792, 8798, 8801, 8805, 8807, 8809, 8810, 8942, 9447, 10110), 63: (5864, 560, 623, 643, 646, 1886, 3933, 3938, 3958, 4538, 5201, 5264, 5284, 5414, 5493, 5552, 5651, 5833, 5840, 5845, 5855, 5857, 5862, 5863, 5866, 5922, 5923, 5924, 5925, 5928, 5929, 5930, 5932, 5933, 5936, 5937, 5938, 5942, 5945, 5946, 5948, 5949, 5951, 5952, 7853, 9842), 64: (7367, 7308, 7343, 7350, 7352, 7353, 7356, 7357, 7364, 7365, 7369, 7371, 7372, 7373, 7396, 7420, 7432, 7445, 7475, 7534, 7577, 7592, 7662, 7734, 7768, 7770, 7773, 7774, 7775, 7776, 7777, 7778, 7821, 7823, 7824, 7828, 7831, 7835, 7841, 7843, 7845, 7848, 7877, 7893, 7949, 9356), 65: (7924, 4609, 7336, 7388, 7413, 7438, 7467, 7482, 7498, 7526, 7541, 7557, 7584, 7599, 7600, 7601, 7606, 7623, 7640, 7654, 7692, 7711, 7757, 7885, 7912, 7913, 7915, 7917, 7918, 7919, 7921, 7922, 7923, 7925, 7927, 7928, 7930, 7931, 7935, 7936, 7937, 7940, 7941, 7947, 8593, 9919), 66: (4735, 1420, 1429, 2083, 4072, 4724, 4725, 4726, 4729, 4730, 4734, 4737, 4740, 4742, 4744, 4745, 4746, 4747, 4749, 4750, 4752, 4753, 4814, 4864, 4904, 4920, 4922, 4923, 4924, 4927, 4931, 4936, 4938, 4940, 4941, 4942, 4944, 4945, 4946, 4970, 5232, 7387, 8050, 9376, 10039), 67: (6852, 222, 880, 881, 886, 888, 893, 901, 902, 907, 908, 909, 910, 912, 913, 1204, 1548, 2211, 2869, 2882, 2895, 2897, 3537, 4200, 4863, 6508, 6708, 6769, 6847, 6855, 6856, 6860, 6861, 6862, 6875, 6877, 6879, 7515, 7834, 8178, 8497, 9504, 10167, 10830), 68: (3406, 91, 197, 198, 519, 588, 613, 2080, 3396, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3507, 3538, 3543, 4069, 5395, 5447, 5917), 69: (8275, 1645, 1668, 7612, 7635, 8015, 8016, 8019, 8027, 8028, 8031, 8036, 8095, 8240, 8270, 8271, 8272, 8273, 8274, 8277, 8278, 8279, 8280, 8282, 8283, 8284, 8285, 8287, 8290, 8291, 8292, 8293, 8296, 8297, 8298, 8300, 8301, 8302, 8303, 8304, 8305, 8540, 9601, 9624), 70: (437, 43, 47, 120, 124, 145, 149, 174, 233, 238, 264, 269, 306, 307, 308, 311, 313, 330, 336, 361, 366, 399, 418, 422, 433, 439, 441, 442, 445, 446, 447, 450, 453, 454, 464, 479, 499, 520, 592, 597, 654, 1763, 4415, 7730), 71: (9855, 2562, 4551, 7866, 9300, 9305, 9306, 9307, 9317, 9389, 9395, 9412, 9417, 9437, 9442, 9468, 9481, 9495, 9500, 9527, 9584, 9597, 9626, 9633, 9654, 9675, 9690, 9694, 9710, 9738, 9741, 9752, 9765, 9769, 9793, 9807, 9845, 9852, 9860, 9897, 9901, 9925, 9941, 10518), 72: (2255, 2033, 2082, 2110, 2135, 2161, 2165, 2220, 2224, 2240, 2241, 2242, 2244, 2246, 2247, 2248, 2250, 2251, 2253, 2254, 2256, 2258, 2260, 2262, 2264, 2265, 2266, 2267, 2317, 2322, 2352, 2366, 2371, 2372, 2375, 2379, 2380, 2381, 2389, 2486, 2490, 2579, 2583, 2618), 73: (7594, 1627, 4279, 7298, 7305, 7310, 7399, 7422, 7447, 7478, 7505, 7525, 7537, 7556, 7572, 7574, 7576, 7579, 7580, 7582, 7583, 7588, 7589, 7590, 7595, 7596, 7597, 7598, 7622, 7636, 7664, 7700, 7731, 7762, 7817, 7856, 7857, 7862, 7863, 7864, 7865, 7896, 7951, 9583), 74: (1874, 11, 421, 428, 1337, 1372, 1397, 1425, 1449, 1474, 1504, 1563, 1621, 1661, 1691, 1739, 1740, 1741, 1743, 1744, 1746, 1748, 1749, 1750, 1751, 1752, 1753, 1756, 1757, 1854, 1856, 1859, 1861, 1863, 1864, 1871, 1876, 1881, 1922, 3736, 3743, 4526, 4652), 75: (10312, 367, 1693, 4345, 5008, 7660, 8323, 9649, 9993, 10041, 10046, 10070, 10095, 10153, 10179, 10210, 10215, 10282, 10296, 10297, 10298, 10299, 10301, 10302, 10303, 10305, 10309, 10310, 10313, 10317, 10318, 10319, 10348, 10368, 10394, 10395, 10425, 10445, 10450, 10538, 10543, 10577, 10582), 76: (1825, 1369, 1421, 1430, 1446, 1471, 1500, 1509, 1531, 1559, 1568, 1590, 1617, 1632, 1633, 1634, 1639, 1656, 1666, 1687, 1725, 1779, 1790, 1805, 1812, 1813, 1814, 1815, 1819, 1822, 1823, 1826, 1828, 1835, 1836, 1838, 1839, 1840, 1846, 1918, 1927, 1980, 4477), 77: (10674, 66, 68, 1532, 2055, 3381, 3383, 3521, 4044, 4707, 4709, 7359, 8162, 10011, 10013, 10151, 10666, 10668, 10673, 10681, 10683, 10685, 10686, 10694, 10700, 10747, 10752, 10809, 10811, 10817, 10819, 10821, 10822, 10824, 10837, 10869, 11099, 11104, 11192, 11197, 11231, 11236), 78: (3322, 1333, 1538, 1996, 3317, 3318, 3319, 3326, 3331, 3332, 3333, 3334, 3335, 3336, 3349, 3357, 3434, 3449, 3459, 3474, 3486, 3515, 3516, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3544, 3576, 3629, 3641, 3665, 3675, 3732, 3894, 3904, 3985, 7300, 9289), 79: (9493, 1537, 2200, 7508, 9298, 9324, 9333, 9374, 9387, 9410, 9426, 9435, 9453, 9466, 9482, 9483, 9484, 9490, 9491, 9494, 9496, 9497, 9498, 9511, 9525, 9608, 9642, 9652, 9699, 9708, 9778, 9791, 9798, 9799, 9803, 9805, 9806, 9809, 9871, 9884, 9910, 9939), 80: (10859, 251, 266, 1577, 3581, 4229, 4892, 4907, 7544, 7559, 8222, 9533, 9548, 10196, 10609, 10611, 10618, 10624, 10627, 10629, 10689, 10701, 10742, 10825, 10839, 10860, 10862, 10865, 10866, 10867, 10875, 10879, 10881, 10884, 10886, 11094, 11105, 11187, 11198, 11226, 11237), 81: (4396, 4021, 4073, 4077, 4098, 4101, 4123, 4156, 4183, 4211, 4215, 4242, 4269, 4273, 4284, 4285, 4286, 4291, 4308, 4313, 4339, 4343, 4377, 4390, 4391, 4393, 4395, 4398, 4400, 4401, 4402, 4406, 4407, 4408, 4409, 4431, 4442, 4481, 4570, 4613, 4632), 82: (9073, 5980, 6264, 6406, 6421, 6447, 6517, 6622, 8410, 8619, 8632, 8646, 8657, 8667, 8735, 8744, 8769, 8799, 8858, 8898, 8909, 8915, 8916, 8918, 8923, 9057, 9058, 9060, 9065, 9068, 9069, 9075, 9077, 9099, 9100, 9140, 9217, 9274, 9275, 9280), 83: (3095, 443, 1769, 2699, 2704, 2776, 2781, 2801, 2806, 2890, 2896, 3085, 3094, 3097, 3098, 3099, 3176, 3216, 3217, 3223, 3224, 3249, 3255, 3310, 3758, 4421, 5979, 6268, 6410, 6449, 6532, 7736, 8399, 8631, 8920, 9062, 9101, 9184, 9188, 9725), 84: (4497, 3994, 4029, 4083, 4096, 4106, 4121, 4131, 4162, 4172, 4173, 4176, 4189, 4206, 4221, 4238, 4251, 4278, 4288, 4303, 4319, 4337, 4348, 4375, 4384, 4394, 4404, 4420, 4430, 4441, 4446, 4473, 4494, 4495, 4498, 4499, 4501, 4532, 4580, 4635), 85: (5207, 1331, 1390, 1892, 4544, 4646, 4658, 4669, 4673, 4681, 4758, 4770, 4783, 4795, 4810, 4826, 4868, 4885, 4900, 4965, 4984, 4999, 5012, 5056, 5068, 5118, 5123, 5151, 5165, 5205, 5210, 5212, 5213, 5228, 5244, 5253, 5293, 5299, 7859, 9848), 86: (4530, 3993, 4081, 4105, 4130, 4160, 4188, 4219, 4250, 4277, 4347, 4383, 4403, 4419, 4445, 4452, 4453, 4455, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4506, 4508, 4509, 4513, 4514, 4516, 4519, 4528, 4533, 4534, 4537, 4540, 4578, 4617, 4634), 87: (4913, 253, 262, 272, 1583, 1598, 2261, 3572, 4656, 4768, 4793, 4813, 4823, 4872, 4882, 4894, 4895, 4896, 4898, 4899, 4903, 4906, 4911, 4914, 4917, 4918, 4919, 4969, 5010, 5066, 5116, 5121, 5122, 5125, 5126, 5138, 5231, 5241, 5297, 10217), 88: (4129, 1477, 2140, 3992, 4027, 4067, 4080, 4104, 4112, 4113, 4114, 4116, 4119, 4120, 4122, 4125, 4128, 4132, 4133, 4135, 4145, 4159, 4187, 4203, 4218, 4236, 4249, 4264, 4276, 4335, 4346, 4437, 4438, 4440, 4447, 4450, 4451, 4471, 4577, 4616), 89: (6950, 320, 744, 746, 750, 751, 752, 757, 764, 769, 770, 771, 772, 774, 1646, 2309, 2972, 3635, 4298, 4961, 5624, 6287, 6711, 6713, 6718, 6719, 6724, 6737, 6738, 6739, 6741, 6742, 6853, 6957, 7613, 8276, 9602, 10265, 10928), 90: (86, 60, 75, 76, 77, 78, 80, 81, 83, 84, 85, 87, 88, 89, 93, 95, 96, 98, 99, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 139, 223, 491, 584, 1413, 4722, 4728, 4748, 10031), 91: (9824, 1582, 1868, 4520, 9454, 9513, 9534, 9535, 9539, 9540, 9541, 9543, 9546, 9547, 9549, 9552, 9553, 9555, 9557, 9558, 9560, 9610, 9779, 9810, 9812, 9817, 9818, 9820, 9823, 9827, 9828, 9830, 9832, 9836, 9837, 9838, 9844, 9872, 9911), 92: (9663, 2370, 9325, 9377, 9402, 9403, 9427, 9428, 9456, 9458, 9487, 9515, 9517, 9573, 9588, 9589, 9590, 9612, 9615, 9643, 9645, 9659, 9666, 9670, 9672, 9673, 9674, 9681, 9682, 9700, 9719, 9746, 9781, 9802, 9874, 9876, 9896, 9913, 9936), 93: (9995, 2039, 9972, 9980, 9981, 9982, 9984, 9985, 9987, 9990, 9991, 9994, 9999, 10000, 10030, 10048, 10072, 10083, 10097, 10155, 10166, 10186, 10200, 10207, 10263, 10350, 10370, 10412, 10419, 10420, 10422, 10429, 10430, 10435, 10528, 10545, 10567, 10584, 10601), 94: (7764, 1797, 2460, 4449, 7307, 7347, 7395, 7402, 7419, 7424, 7444, 7474, 7481, 7502, 7507, 7533, 7540, 7564, 7569, 7631, 7639, 7661, 7666, 7702, 7717, 7722, 7738, 7752, 7753, 7754, 7755, 7766, 7903, 7904, 7905, 7908, 7910, 7953, 10416), 95: (1686, 1418, 1470, 1497, 1525, 1526, 1527, 1528, 1533, 1534, 1535, 1536, 1540, 1541, 1542, 1555, 1560, 1587, 1591, 1615, 1652, 1676, 1677, 1679, 1683, 1684, 1692, 1694, 1695, 1696, 1697, 1699, 1700, 1915, 1919, 1954, 4338, 7653, 8316), 96: (10708, 97, 100, 1426, 2086, 2089, 4075, 4078, 4738, 4741, 5401, 7390, 7393, 8056, 9379, 9382, 10638, 10640, 10691, 10692, 10696, 10697, 10702, 10706, 10707, 10710, 10711, 10712, 10713, 10714, 10716, 10717, 10718, 10719, 10720, 10831, 11022, 11244), 97: (8064, 1434, 2097, 4086, 7401, 8037, 8039, 8040, 8041, 8044, 8045, 8048, 8052, 8057, 8058, 8059, 8063, 8065, 8067, 8068, 8143, 8179, 8202, 8538, 8539, 8541, 8542, 8543, 8546, 8549, 8553, 8554, 8556, 8557, 8563, 8564, 9390, 10053), 98: (4606, 2617, 4020, 4070, 4088, 4167, 4177, 4179, 4180, 4185, 4186, 4190, 4191, 4192, 4193, 4194, 4226, 4239, 4267, 4304, 4325, 4474, 4567, 4596, 4597, 4598, 4599, 4602, 4604, 4607, 4608, 4610, 4615, 4620, 4621, 4622, 4625, 4626), 99: (203, 7, 42, 92, 119, 144, 171, 192, 199, 200, 201, 202, 205, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 228, 229, 261, 326, 360, 397, 417, 463, 495, 496, 589, 2192, 4181, 5507), 100: (1387, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1368, 1371, 1373, 1374, 1375, 1377, 1380, 1381, 1383, 1385, 1386, 1389, 1398, 1402, 1404, 1405, 1406, 1411, 1464, 1465, 1489, 1547, 1581, 1609, 1644, 1680, 1816, 1909, 1948), 101: (7415, 2111, 4100, 7332, 7381, 7406, 7407, 7408, 7409, 7410, 7416, 7417, 7418, 7423, 7425, 7440, 7459, 7517, 7529, 7550, 7560, 7614, 7627, 7649, 7657, 7686, 7687, 7689, 7696, 7701, 7703, 7704, 7713, 7878, 7888, 9404, 10067), 102: (4561, 2572, 3898, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4023, 4025, 4026, 4032, 4033, 4141, 4165, 4199, 4224, 4233, 4261, 4296, 4323, 4468, 4557, 4558, 4560, 4563, 4565, 4568, 4573, 4575, 4576, 4581, 4583, 4585, 4586), 103: (5578, 274, 1600, 3589, 4252, 4915, 5397, 5410, 5458, 5516, 5535, 5548, 5556, 5561, 5562, 5563, 5565, 5571, 5575, 5577, 5580, 5582, 5711, 5773, 5801, 5814, 5828, 5867, 5875, 5876, 5894, 5907, 5962, 7567, 8230, 9556), 104: (10020, 1401, 9357, 9996, 10023, 10025, 10050, 10073, 10084, 10098, 10129, 10156, 10188, 10218, 10245, 10286, 10315, 10351, 10371, 10413, 10436, 10454, 10460, 10461, 10462, 10466, 10468, 10469, 10470, 10471, 10472, 10529, 10547, 10568, 10586, 10602), 105: (5081, 126, 424, 440, 461, 466, 467, 1624, 1766, 1787, 1788, 2429, 4418, 4690, 4767, 4792, 4822, 4881, 4910, 4912, 4939, 4979, 5009, 5065, 5078, 5101, 5102, 5103, 5108, 5109, 5110, 5112, 5113, 5114, 5240), 106: (6525, 1884, 1910, 3231, 4536, 6154, 6213, 6312, 6497, 6502, 6503, 6523, 6526, 6529, 6546, 6550, 6552, 6553, 6564, 6565, 6569, 6570, 6572, 6573, 6574, 6575, 7851, 8514, 8535, 8561, 9198, 9224, 9840, 9866, 9887), 107: (7846, 1879, 4531, 7397, 7476, 7535, 7633, 7779, 7780, 7781, 7783, 7785, 7786, 7787, 7789, 7790, 7791, 7792, 7793, 7795, 7796, 7798, 7803, 7804, 7805, 7806, 7807, 7829, 7838, 7849, 7852, 7855, 7894, 8465, 9835), 108: (8362, 406, 1732, 2395, 7303, 7972, 8007, 8061, 8084, 8103, 8109, 8140, 8192, 8199, 8223, 8229, 8320, 8326, 8349, 8352, 8364, 8367, 8398, 8458, 8472, 8473, 8477, 8479, 8482, 8483, 8510, 8558, 8590, 8613, 9688), 109: (470, 14, 18, 49, 151, 155, 181, 186, 240, 245, 271, 275, 338, 344, 368, 372, 459, 460, 468, 469, 471, 472, 473, 525, 570, 578, 599, 615, 638, 659, 2459, 4448, 7763, 8426, 10415), 110: (315, 23, 24, 26, 35, 135, 160, 218, 314, 316, 317, 318, 322, 323, 328, 331, 337, 343, 345, 346, 348, 349, 351, 393, 515, 580, 609, 619, 649, 1641, 2304, 4293, 5619, 7608, 10260), 111: (4955, 1640, 1669, 2303, 4292, 4641, 4643, 4644, 4654, 4657, 4661, 4662, 4675, 4775, 4800, 4957, 4958, 4959, 4962, 4963, 4964, 4968, 4972, 4976, 4978, 4981, 4986, 4987, 4989, 4990, 4991, 5127, 5220, 7607, 9596), 112: (8391, 1761, 2424, 4413, 7728, 7961, 7968, 7981, 7984, 7996, 8003, 8073, 8080, 8098, 8125, 8158, 8183, 8194, 8215, 8243, 8252, 8314, 8322, 8389, 8394, 8397, 8400, 8401, 8402, 8403, 8433, 8450, 8568, 8582, 8608), 113: (3388, 73, 102, 1340, 3329, 3364, 3393, 3395, 3441, 3466, 3496, 3524, 3555, 3586, 3613, 3653, 3683, 3719, 3739, 3755, 3775, 3776, 3783, 3784, 3785, 3786, 3787, 3788, 3806, 3914, 4655, 4743, 5111, 10018), 114: (9353, 9294, 9329, 9339, 9341, 9342, 9343, 9345, 9354, 9358, 9360, 9361, 9362, 9406, 9421, 9431, 9489, 9520, 9566, 9578, 9618, 9648, 9715, 9717, 9720, 9721, 9723, 9724, 9726, 9727, 9728, 9729, 9813, 9879), 115: (10570, 1951, 2614, 4603, 8581, 9907, 10034, 10055, 10080, 10081, 10086, 10087, 10089, 10092, 10096, 10102, 10170, 10193, 10203, 10292, 10438, 10564, 10565, 10566, 10569, 10571, 10573, 10574, 10580, 10583, 10589, 10591, 10592, 10593), 116: (6259, 1618, 4270, 4933, 6140, 6142, 6199, 6201, 6246, 6248, 6250, 6253, 6263, 6267, 6270, 6271, 6272, 6319, 6323, 6324, 6333, 6334, 6337, 6340, 6341, 6558, 6560, 7585, 8248, 8318, 9574, 9644, 10237), 117: (2000, 1999, 2003, 2035, 2088, 2112, 2148, 2167, 2226, 2324, 2339, 2354, 2401, 2402, 2404, 2406, 2407, 2409, 2410, 2411, 2413, 2414, 2415, 2417, 2418, 2419, 2420, 2492, 2585, 2624, 3989, 7304, 9293), 118: (9787, 4483, 9295, 9330, 9383, 9407, 9432, 9462, 9465, 9524, 9579, 9619, 9622, 9685, 9705, 9731, 9736, 9762, 9768, 9770, 9772, 9775, 9776, 9780, 9782, 9784, 9785, 9792, 9794, 9796, 9880, 9883, 9937), 119: (9620, 1664, 4316, 9296, 9384, 9408, 9433, 9463, 9522, 9598, 9599, 9600, 9603, 9604, 9605, 9609, 9611, 9613, 9616, 9617, 9625, 9627, 9628, 9629, 9630, 9631, 9650, 9706, 9744, 9751, 9755, 9920, 10283), 120: (8669, 50, 1376, 2688, 2707, 2935, 4028, 4691, 6014, 8006, 8634, 8653, 8655, 8658, 8666, 8670, 8671, 8673, 8674, 8675, 8746, 8757, 8771, 8840, 8860, 8902, 9094, 9103, 9104, 9202, 9219, 9332), 121: (6425, 458, 1784, 2957, 4436, 5099, 5988, 5996, 6015, 6023, 6092, 6100, 6117, 6147, 6158, 6206, 6217, 6316, 6414, 6416, 6488, 6494, 6539, 6584, 6629, 7751, 8414, 8924, 9066, 9595, 9740, 10403), 122: (5481, 177, 1503, 2166, 4155, 5402, 5426, 5451, 5464, 5471, 5475, 5487, 5488, 5492, 5495, 5530, 5540, 5638, 5668, 5697, 5698, 5699, 5700, 5710, 5713, 5714, 5715, 5724, 5806, 7470, 8133, 9459), 123: (2361, 2085, 2096, 2119, 2202, 2223, 2234, 2333, 2340, 2343, 2345, 2346, 2347, 2349, 2351, 2353, 2355, 2357, 2358, 2359, 2362, 2363, 2397, 2489, 2500, 2567, 2582, 2621, 2632, 4350, 7665, 8328), 124: (7889, 4574, 7392, 7471, 7480, 7530, 7539, 7628, 7638, 7658, 7705, 7706, 7708, 7710, 7716, 7721, 7723, 7724, 7872, 7875, 7876, 7880, 7882, 7883, 7886, 7890, 7891, 7898, 7900, 7901, 9878, 10541), 125: (2466, 1803, 2014, 2017, 2029, 2092, 2106, 2116, 2131, 2141, 2191, 2199, 2216, 2230, 2385, 2394, 2451, 2456, 2464, 2473, 2474, 2483, 2496, 2511, 2576, 2589, 2601, 2615, 2628, 2641, 2645, 9759), 126: (11115, 501, 507, 1833, 4479, 4485, 5148, 5805, 7794, 7800, 8457, 9783, 9789, 10452, 10850, 10948, 11095, 11096, 11097, 11100, 11101, 11102, 11110, 11111, 11117, 11118, 11119, 11120, 11121, 11122, 11209), 127: (414, 3, 30, 31, 33, 37, 46, 137, 162, 178, 220, 237, 335, 353, 365, 395, 413, 420, 426, 429, 430, 431, 517, 582, 596, 611, 621, 651, 4392, 5055, 10359), 128: (5227, 586, 605, 1912, 2575, 3901, 3920, 4564, 4777, 4779, 4782, 4794, 4798, 4808, 4828, 4866, 4887, 4998, 5134, 5224, 5226, 5234, 5237, 5242, 5243, 5246, 5248, 5249, 7879, 9868, 10531), 129: (5313, 1998, 3987, 5305, 5311, 5314, 5318, 5320, 5323, 5325, 5385, 5425, 5438, 5450, 5521, 5539, 5570, 5637, 5647, 5681, 5691, 5692, 5694, 5696, 5704, 5790, 5883, 5898, 7302, 9291), 130: (3742, 427, 3361, 3438, 3444, 3469, 3493, 3500, 3552, 3559, 3650, 3658, 3680, 3686, 3728, 3729, 3741, 3745, 3746, 3839, 3879, 3884, 3885, 3887, 3911, 3918, 3973, 4405, 7720, 9709), 131: (2325, 1662, 2036, 2113, 2196, 2227, 2305, 2307, 2310, 2311, 2315, 2316, 2320, 2323, 2327, 2328, 2329, 2334, 2335, 2336, 2337, 2338, 2391, 2421, 2422, 2434, 2436, 2586, 4314, 7629), 132: (1916, 1419, 1498, 1513, 1557, 1572, 1580, 1584, 1585, 1589, 1595, 1597, 1599, 1602, 1604, 1671, 1905, 1908, 1911, 1913, 1921, 1923, 1924, 1925, 1926, 1929, 1931, 1933, 1934, 1955), 133: (5091, 1776, 4428, 4671, 4672, 4674, 4678, 4689, 4756, 4766, 4778, 4791, 4803, 4821, 4861, 4994, 5064, 5090, 5095, 5130, 5158, 5162, 5223, 5239, 5252, 5292, 5296, 7743, 9732), 134: (3490, 175, 362, 1501, 1688, 3477, 3478, 3481, 3482, 3483, 3499, 3502, 3506, 3541, 3549, 3638, 3668, 3669, 3671, 3672, 3677, 3685, 3908, 4153, 4816, 5003, 7468, 8131, 9457), 135: (10222, 1603, 4255, 7570, 10038, 10056, 10077, 10160, 10176, 10194, 10199, 10202, 10204, 10206, 10212, 10216, 10221, 10223, 10355, 10375, 10417, 10535, 10552, 10594, 10596, 10597, 10605, 10606, 10607), 136: (9526, 1570, 4222, 9299, 9388, 9411, 9436, 9502, 9503, 9505, 9507, 9508, 9509, 9512, 9514, 9518, 9519, 9523, 9529, 9530, 9531, 9532, 9689, 9846, 9851, 9853, 9854, 9924, 9940), 137: (7005, 375, 684, 1121, 1225, 1325, 1701, 2364, 3027, 3690, 4353, 5016, 5679, 6342, 6651, 6710, 6788, 6864, 6880, 6935, 7208, 7247, 7292, 7668, 8331, 8994, 9657, 10320), 138: (2708, 56, 1382, 2045, 2673, 2681, 2685, 2691, 2703, 2706, 2785, 2790, 2873, 2902, 3179, 3235, 3260, 3269, 3314, 3371, 4034, 4697, 7349, 8012, 8648, 8968, 9338, 10001), 139: (5423, 1445, 2108, 4097, 5392, 5396, 5417, 5432, 5435, 5437, 5448, 5503, 5504, 5506, 5515, 5518, 5519, 5520, 5528, 5533, 5630, 5664, 5721, 5800, 7412, 8075, 9401, 10064), 140: (482, 0, 17, 152, 154, 182, 185, 241, 244, 369, 371, 425, 475, 480, 484, 485, 524, 564, 569, 571, 572, 600, 603, 642, 656, 658, 1808, 9764), 141: (270, 13, 48, 150, 172, 180, 231, 254, 255, 258, 265, 267, 273, 276, 277, 278, 403, 423, 449, 521, 590, 598, 629, 655, 1596, 4248, 7563, 8226), 142: (2163, 2149, 2150, 2152, 2155, 2156, 2157, 2158, 2160, 2164, 2170, 2171, 2172, 2173, 2176, 2177, 2178, 2179, 2180, 2194, 2215, 2222, 2312, 2319, 2350, 2388, 2453, 4152), 143: (9470, 188, 1514, 4166, 9336, 9391, 9413, 9441, 9443, 9444, 9445, 9448, 9449, 9450, 9451, 9460, 9464, 9471, 9472, 9473, 9585, 9691, 9711, 9766, 9894, 9898, 9899, 9942), 144: (4411, 1759, 3980, 3990, 4001, 4005, 4006, 4008, 4061, 4092, 4102, 4139, 4216, 4247, 4259, 4268, 4274, 4344, 4380, 4410, 4416, 4422, 4423, 4424, 4425, 4427, 4542, 4628), 145: (6019, 52, 1378, 2686, 3367, 4030, 4693, 5984, 6001, 6003, 6006, 6021, 6022, 6096, 6105, 6121, 6130, 6152, 6188, 6211, 6310, 6491, 6531, 6538, 6625, 7345, 9334), 146: (3929, 614, 1938, 1940, 3369, 3446, 3460, 3503, 3548, 3562, 3591, 3623, 3661, 3688, 3744, 3841, 3886, 3907, 3921, 3925, 3926, 3930, 3931, 3932, 3975, 4592, 7907), 147: (10515, 4548, 10051, 10052, 10075, 10099, 10100, 10157, 10158, 10189, 10190, 10220, 10288, 10289, 10352, 10353, 10414, 10455, 10456, 10508, 10516, 10517, 10523, 10548, 10560, 10587, 10588), 148: (622, 34, 36, 38, 51, 54, 55, 138, 163, 189, 221, 248, 354, 583, 618, 625, 628, 632, 641, 644, 647, 648, 1673, 2611, 4600, 8578, 9904), 149: (5892, 2577, 4566, 5412, 5491, 5502, 5532, 5550, 5649, 5701, 5767, 5799, 5823, 5884, 5889, 5890, 5891, 5893, 5897, 5899, 5903, 5905, 5906, 5910, 5911, 5912), 150: (5769, 465, 1791, 4443, 5105, 5106, 5406, 5430, 5455, 5485, 5513, 5544, 5642, 5672, 5708, 5728, 5763, 5764, 5765, 5766, 5772, 5775, 5776, 5777, 7758, 10410), 151: (8375, 2408, 8102, 8104, 8134, 8191, 8193, 8319, 8321, 8348, 8356, 8368, 8369, 8371, 8373, 8376, 8380, 8382, 8384, 8387, 8459, 8550, 8552, 8591, 9701, 11037), 152: (8836, 217, 1543, 2206, 4195, 4858, 7510, 8173, 8186, 8621, 8635, 8636, 8639, 8640, 8753, 8838, 8843, 8844, 8845, 8862, 8863, 8865, 8866, 8868, 8869, 9499), 153: (3974, 3370, 3445, 3447, 3470, 3472, 3501, 3505, 3560, 3564, 3592, 3659, 3663, 3687, 3689, 3840, 3893, 3922, 3965, 3966, 3972, 3976, 3977, 4637, 7952, 10604), 154: (8417, 4439, 7959, 7986, 7987, 7988, 7989, 7993, 8071, 8093, 8107, 8118, 8157, 8165, 8176, 8196, 8227, 8309, 8415, 8418, 8429, 8567, 8594, 8607, 9743, 10406), 155: (10261, 1642, 4294, 7609, 9968, 9973, 9975, 10028, 10059, 10145, 10262, 10266, 10271, 10272, 10273, 10279, 10280, 10285, 10290, 10291, 10293, 10294, 10358, 10405, 10433, 10595), 156: (9875, 4571, 9469, 9516, 9528, 9635, 9639, 9640, 9646, 9647, 9651, 9653, 9655, 9656, 9861, 9864, 9865, 9867, 9869, 9877, 9882, 9885, 9889, 9890, 9914, 9926), 157: (4386, 3988, 3996, 4076, 4085, 4164, 4214, 4223, 4245, 4253, 4280, 4312, 4322, 4342, 4371, 4373, 4374, 4381, 4385, 4387, 4388, 4389, 4434, 4555, 4556, 4593), 158: (11173, 565, 1891, 3880, 4543, 5206, 7858, 8521, 9847, 10510, 10641, 10723, 10745, 10762, 10828, 10852, 11003, 11179, 11180, 11190, 11211, 11219, 11229, 11250, 11259), 159: (2054, 1995, 2030, 2107, 2132, 2159, 2182, 2183, 2184, 2186, 2187, 2217, 2249, 2277, 2299, 2314, 2348, 2386, 2405, 2452, 2467, 2484, 2508, 2602, 2642), 160: (3609, 402, 405, 1620, 1728, 1731, 2283, 3360, 3437, 3462, 3492, 3547, 3551, 3582, 3649, 3679, 3709, 3710, 3721, 3724, 3726, 3735, 3910, 4272, 10239), 161: (3568, 1579, 3342, 3345, 3351, 3429, 3451, 3476, 3487, 3534, 3546, 3569, 3570, 3573, 3580, 3588, 3593, 3631, 3643, 3831, 3905, 4231, 7546, 8209, 10198), 162: (8208, 4230, 7545, 7979, 7980, 7991, 8069, 8091, 8116, 8128, 8155, 8174, 8187, 8213, 8214, 8217, 8221, 8231, 8234, 8307, 8443, 8453, 8575, 8585, 10197), 163: (1237, 574, 681, 818, 1241, 1267, 1278, 1900, 3226, 3889, 4552, 5215, 6541, 6648, 6785, 6932, 7207, 7245, 7867, 8530, 9193, 9856, 10519, 11182), 164: (9194, 575, 3890, 4553, 5216, 7868, 8531, 8622, 8649, 8652, 8656, 8734, 8749, 8756, 8839, 8864, 8963, 9136, 9195, 9201, 9230, 9270, 9857, 10520), 165: (5986, 19, 1345, 2671, 3997, 4660, 5969, 5971, 5987, 6098, 6101, 6184, 6214, 6577, 6583, 6627, 7312, 7975, 8515, 8572, 8638, 9235, 9301, 9841), 166: (10556, 1937, 4589, 9893, 9969, 9977, 9978, 10029, 10054, 10076, 10146, 10159, 10165, 10192, 10340, 10354, 10374, 10434, 10457, 10527, 10554, 10557, 10561, 10562), 167: (6786, 156, 797, 804, 817, 1482, 2145, 2808, 3471, 4134, 4797, 5460, 6123, 6764, 6771, 7449, 7819, 7909, 8112, 8775, 9438, 10101, 10764), 168: (6308, 341, 1667, 2330, 3656, 4982, 5645, 5983, 6095, 6120, 6151, 6210, 6289, 6294, 6313, 6409, 6483, 6490, 6493, 6624, 7624, 8960, 9623), 169: (5817, 2502, 4491, 5154, 5408, 5415, 5461, 5546, 5553, 5644, 5791, 5792, 5797, 5798, 5813, 5816, 5818, 5953, 5964, 5966, 8469, 9795, 10458), 170: (4324, 1672, 4087, 4109, 4254, 4281, 4295, 4299, 4301, 4305, 4307, 4309, 4315, 4321, 4326, 4327, 4351, 4549, 4587, 4590, 4594, 4595, 4638), 171: (2656, 4, 1330, 2654, 2766, 2780, 2783, 2784, 2786, 2792, 2876, 3008, 3237, 3982, 4645, 6083, 7297, 7960, 8072, 8623, 8748, 9286), 172: (8506, 8004, 8081, 8106, 8136, 8246, 8253, 8395, 8405, 8406, 8412, 8436, 8461, 8484, 8486, 8487, 8491, 8492, 8494, 8511, 8518, 8611), 173: (7732, 1765, 4417, 7306, 7340, 7341, 7394, 7443, 7473, 7500, 7501, 7531, 7562, 7630, 7659, 7725, 7726, 7739, 7740, 7742, 7747, 7948), 174: (1120, 457, 682, 813, 819, 1279, 1280, 1783, 2446, 3772, 4435, 5098, 6424, 6649, 6933, 7246, 7750, 8413, 9076, 9739, 10402), 175: (11126, 1844, 2507, 4496, 5159, 8474, 9800, 10463, 10736, 10750, 10761, 10835, 10851, 11004, 11014, 11034, 11134, 11135, 11195, 11210, 11220), 176: (6146, 179, 1505, 2168, 4157, 6091, 6116, 6126, 6128, 6133, 6134, 6155, 6157, 6193, 6205, 6400, 6412, 6413, 7472, 8135, 9461), 177: (8454, 4476, 8188, 8198, 8235, 8237, 8239, 8242, 8255, 8258, 8259, 8260, 8261, 8444, 8446, 8449, 8452, 8456, 8467, 8470, 8586), 178: (10546, 2590, 4579, 10049, 10187, 10191, 10432, 10439, 10441, 10447, 10448, 10453, 10459, 10525, 10530, 10532, 10534, 10540, 10544, 10550, 10553), 179: (7646, 4331, 7296, 7317, 7323, 7324, 7326, 7330, 7377, 7455, 7494, 7513, 7527, 7558, 7610, 7644, 7650, 7651, 7655, 7667, 7944), 180: (7519, 226, 1552, 4204, 7452, 7453, 7454, 7456, 7460, 7461, 7462, 7483, 7484, 7511, 7514, 7516, 7518, 7520, 7523, 7542, 7543), 181: (2598, 1990, 2012, 2015, 2024, 2043, 2098, 2102, 2120, 2124, 2188, 2203, 2207, 2382, 2398, 2463, 2476, 2501, 2606, 2638, 2649), 182: (1767, 1338, 1450, 1453, 1478, 1564, 1567, 1616, 1622, 1625, 1764, 1768, 1771, 1772, 1773, 1801, 1806, 1807, 1810, 1811, 1982), 183: (1224, 663, 683, 820, 1199, 1222, 1322, 1323, 1324, 1887, 4539, 6528, 6650, 6787, 7289, 7290, 7291, 7854, 8517, 9843), 184: (11056, 448, 1774, 4426, 7741, 8404, 9730, 10393, 10631, 10632, 10633, 10743, 10826, 10847, 11001, 11020, 11067, 11171, 11181, 11227), 185: (2891, 239, 1565, 2871, 2872, 2877, 2886, 2898, 2899, 2901, 4217, 4880, 6197, 7532, 8195, 8849, 8851, 8857, 9521, 10184), 186: (8768, 1475, 2138, 4127, 6108, 7442, 8105, 8743, 8755, 8759, 8760, 8772, 8773, 8776, 8786, 8905, 9052, 9064, 9205, 9216), 187: (4997, 1682, 4334, 4680, 4755, 4757, 4764, 4769, 4772, 4773, 4807, 4875, 4995, 5006, 5011, 5014, 5015, 5141, 8312, 9638), 188: (8310, 4332, 7647, 7990, 7992, 7994, 7995, 8010, 8011, 8094, 8119, 8177, 8190, 8306, 8313, 8325, 8329, 8330, 8588, 9636), 189: (8127, 4149, 7464, 8100, 8115, 8117, 8122, 8123, 8138, 8142, 8144, 8145, 8146, 8147, 8156, 8169, 8172, 8182, 8185, 8584), 190: (9367, 2074, 4063, 7378, 9316, 9318, 9319, 9320, 9322, 9337, 9363, 9365, 9366, 9369, 9371, 9385, 9393, 9394, 9420, 9565), 191: (11012, 404, 1730, 2393, 4382, 7697, 8360, 9686, 10349, 10755, 10759, 10844, 10848, 11017, 11018, 11019, 11028, 11032, 11081), 192: (8752, 1459, 2122, 3448, 4111, 4774, 7426, 8089, 8733, 8747, 8750, 8751, 8785, 9146, 9191, 9236, 9281, 9415, 10078), 193: (394, 2, 27, 45, 136, 147, 161, 236, 334, 364, 409, 411, 516, 595, 650, 2383, 4372, 9676, 10339), 194: (1649, 1485, 1487, 1492, 1493, 1494, 1495, 1506, 1507, 1508, 1516, 1517, 1643, 1647, 1655, 1657, 1663, 1674, 1675), 195: (11212, 604, 3919, 4582, 7897, 9886, 10549, 10853, 10854, 11193, 11194, 11203, 11204, 11214, 11215, 11216, 11251, 11252), 196: (4487, 2498, 4082, 4220, 4318, 4464, 4465, 4466, 4467, 4472, 4478, 4480, 4488, 4489, 4490, 4492, 4502, 4505), 197: (2633, 1970, 2099, 2235, 2237, 2569, 2574, 2578, 2584, 2588, 2596, 2597, 2608, 2609, 2613, 2627, 2636, 2637), 198: (2457, 1794, 2038, 2040, 2091, 2094, 2115, 2117, 2142, 2198, 2229, 2232, 2449, 2455, 2461, 2462, 2505, 2516), 199: (821, 158, 1271, 1484, 2147, 2810, 3473, 4136, 4799, 5462, 6125, 7451, 8114, 8777, 9440, 10103, 10766), 200: (2952, 1626, 2805, 2884, 2933, 2938, 2955, 3168, 3178, 3243, 3254, 3309, 8256, 8919, 9061, 9142, 9582), 201: (3258, 606, 1932, 2595, 3234, 3238, 3259, 3262, 3263, 3312, 4584, 5247, 7899, 8562, 9225, 9888, 10551), 202: (4824, 4161, 4802, 4804, 4809, 4819, 4825, 4829, 4831, 4832, 4867, 4883, 5144, 5150, 5153, 5155, 8139), 203: (3430, 1441, 3346, 3348, 3352, 3354, 3431, 3443, 3535, 3632, 3636, 3832, 3897, 3900, 4093, 9397, 10060), 204: (10404, 1785, 2448, 9970, 9971, 10047, 10058, 10071, 10144, 10154, 10163, 10185, 10338, 10357, 10369, 10407, 10418), 205: (7621, 1654, 4306, 7386, 7465, 7524, 7548, 7551, 7552, 7554, 7571, 7611, 7616, 7619, 7620, 7641, 7642), 206: (7493, 2189, 4178, 7295, 7316, 7320, 7321, 7329, 7335, 7376, 7385, 7429, 7437, 7492, 7495, 7509, 7943), 207: (8943, 1650, 2313, 4302, 7617, 8736, 8761, 8846, 8941, 8946, 8962, 8964, 8965, 9113, 9606, 10269), 208: (10637, 29, 1355, 2018, 3344, 4007, 4670, 5333, 7322, 7985, 9311, 9974, 10741, 10827, 10858, 11270), 209: (6107, 1466, 2129, 3455, 4118, 4781, 6081, 6099, 6103, 6124, 6191, 6192, 7433, 8096, 9422, 10085), 210: (5914, 610, 1936, 2599, 4588, 5251, 5327, 5551, 5650, 5820, 5913, 5916, 5921, 8566, 9892, 10555), 211: (4687, 4024, 4677, 4679, 4692, 4695, 4696, 4862, 4878, 5054, 5067, 5070, 5071, 5072, 7339, 9328), 212: (350, 16, 20, 21, 134, 159, 234, 357, 370, 373, 374, 579, 593, 4328, 7643, 9632), 213: (9314, 1358, 4010, 7325, 9285, 9287, 9310, 9312, 9313, 9315, 9424, 9569, 9678, 9908, 9933, 9934), 214: (4201, 1549, 2212, 4059, 4062, 4065, 4079, 4084, 4089, 4090, 4196, 4205, 4208, 4210, 4227, 4228), 215: (1964, 1428, 1436, 1452, 1566, 1574, 1786, 1793, 1795, 1798, 1799, 1944, 1958, 1968, 1973, 1974), 216: (1882, 1343, 1432, 1455, 1480, 1850, 1857, 1862, 1880, 1885, 1888, 1890, 1895, 1897, 1898, 1984), 217: (10756, 148, 2137, 3463, 4126, 4789, 7441, 9430, 10093, 10731, 10748, 10765, 10833, 10845, 11039), 218: (1723, 1424, 1448, 1554, 1562, 1593, 1660, 1690, 1720, 1722, 1729, 1736, 1789, 1821, 7690), 219: (9145, 526, 1852, 2515, 4504, 5167, 5830, 8803, 9135, 9143, 9190, 9229, 9276, 9279, 9808), 220: (5298, 523, 657, 1849, 1983, 3838, 4884, 4890, 5157, 5164, 5168, 5291, 5301, 5302, 5303), 221: (8203, 247, 1573, 2236, 4225, 4888, 8087, 8181, 8197, 8201, 8205, 8206, 8440, 8573, 8616), 222: (4243, 4071, 4150, 4209, 4212, 4232, 4235, 4237, 4241, 4256, 4329, 4336, 4340, 4349, 4352), 223: (4140, 1488, 3981, 4002, 4009, 4011, 4137, 4138, 4144, 4146, 4147, 4158, 4168, 4169, 4629), 224: (2491, 2034, 2087, 2093, 2136, 2225, 2231, 2390, 2399, 2400, 2477, 2479, 2482, 2485, 2503), 225: (745, 82, 775, 1408, 2071, 3397, 4060, 4723, 5386, 7375, 8038, 9364, 10027, 10690), 226: (9210, 1917, 4569, 7884, 8547, 8806, 8900, 8922, 9204, 9221, 9222, 9226, 9227, 9873), 227: (5400, 1422, 4074, 5387, 5393, 5409, 5413, 5416, 5527, 5538, 5655, 5677, 7389, 9378), 228: (6186, 219, 1545, 2208, 4197, 4860, 5523, 5994, 6216, 6617, 7512, 8175, 9501, 10164), 229: (7818, 4503, 7311, 7398, 7400, 7448, 7479, 7506, 7538, 7568, 7637, 7737, 7749, 7871), 230: (7431, 2127, 3453, 7327, 7328, 7331, 7333, 7348, 7382, 7427, 7428, 7434, 7435, 7450), 231: (1479, 1342, 1415, 1431, 1454, 1460, 1461, 1467, 1468, 1483, 1569, 1842, 1843, 1853), 232: (7136, 506, 1832, 2495, 3821, 4484, 5147, 5810, 7799, 8462, 9788, 10451, 11114), 233: (3228, 2565, 2769, 2879, 3010, 4554, 5217, 7869, 8532, 8788, 9346, 9858, 10521), 234: (10365, 9702, 10043, 10044, 10068, 10181, 10182, 10341, 10346, 10356, 10360, 10362, 10376), 235: (9405, 9321, 9381, 9396, 9398, 9399, 9409, 9414, 9695, 9697, 9713, 9906, 9917), 236: (4257, 1605, 3978, 3979, 3983, 3995, 3998, 3999, 4265, 4282, 4283, 4547, 9561), 237: (1623, 1339, 1427, 1476, 1607, 1612, 1613, 1629, 1630, 1775, 1780, 1847, 1981), 238: (11253, 645, 1971, 2634, 4623, 9927, 10590, 10739, 10855, 10856, 11225, 11256), 239: (3004, 352, 1678, 2341, 2788, 3302, 3667, 4330, 4993, 7645, 8308, 9634), 240: (10174, 2218, 4207, 7522, 10037, 10147, 10149, 10161, 10168, 10172, 10175, 10195), 241: (2214, 225, 1551, 2078, 2125, 2133, 2146, 2210, 2213, 2219, 2238, 2239), 242: (7257, 627, 1953, 2616, 3942, 4605, 5931, 7125, 9246, 10572, 11235), 243: (5626, 1648, 3637, 4300, 5439, 5457, 5529, 5631, 5653, 7615, 10267), 244: (4103, 1451, 3991, 4091, 4094, 4095, 4107, 4110, 4275, 4432, 4633), 245: (3917, 602, 1928, 2591, 3366, 3468, 3558, 3561, 3660, 3842, 3923), 246: (9587, 1631, 9303, 9563, 9572, 9581, 9586, 9767, 9912, 9930, 9944), 247: (5819, 2504, 4493, 5328, 5329, 5330, 5522, 5547, 5716, 5831), 248: (8597, 1967, 2630, 4619, 8204, 8576, 8580, 8599, 8604, 9923), 249: (3642, 327, 1653, 3536, 3540, 3545, 3565, 3633, 3646, 3664), 250: (8092, 1462, 7958, 7983, 8090, 8097, 8108, 8113, 8431, 9418), 251: (9429, 1473, 9292, 9416, 9423, 9434, 9439, 9568, 9692, 9693), 252: (10832, 1550, 3539, 4202, 4865, 8180, 9506, 10169, 10857), 253: (10033, 1414, 2077, 4066, 9370, 10032, 10057, 10061, 10062), 254: (7971, 1341, 7956, 7976, 7977, 8437, 8441, 8612, 9297), 255: (1737, 1347, 1359, 1438, 1576, 1719, 1721, 1735, 1988), 256: (2789, 1463, 2809, 3452, 4115, 7430, 9419, 10082), 257: (8086, 4108, 8070, 8083, 8088, 8111, 8439, 8534), 258: (2081, 2031, 2072, 2100, 2101, 2193, 2204, 2205), 259: (246, 187, 230, 243, 249, 250, 607, 608), 260: (184, 153, 167, 168, 190, 191, 527, 1510), 261: (6601, 634, 1960, 2623, 4612, 9916, 10579), 262: (8804, 1511, 4163, 8780, 9183, 9277, 9467), 263: (7319, 1352, 4004, 7314, 7318, 7405, 7955), 264: (9284, 1328, 9282, 9302, 9309, 9757, 9902), 265: (1945, 1349, 1350, 1439, 1486, 1544, 1975), 266: (1553, 1354, 1443, 1556, 1575, 1867, 1978), 267: (8811, 1518, 2181, 4170, 5496, 7485), 268: (5524, 1546, 4198, 5337, 5531, 5554), 269: (157, 141, 660, 661, 662, 3456), 270: (7374, 1407, 7293, 7313, 7380, 7404), 271: (1457, 1440, 1442, 1458, 1942, 1986), 272: (5258, 617, 1943, 4891, 5257), 273: (2028, 2025, 2044, 2103, 2121), 274: (1433, 1409, 1410, 1435, 1437), 275: (1356, 1329, 1353, 1357, 1977), 276: (9269, 1976, 8606, 9932), 277: (4550, 4541, 4636, 4640), 278: (9116, 4475, 10442), 279: (4873, 1558, 4871), 280: (4003, 4627, 4630), 281: (9938, 9763, 9943), 282: (2651, 2639, 8618), 283: (1346, 1326, 1987), 284: (7945, 7954), 285: (6630,), 286: (5735,), 287: (3353,), 288: (616,), 289: (4639,), 290: (4668,), 291: (8617,)}\n",
      "[383, 354, 307, 191, 187, 182, 158, 148, 146, 140, 133, 130, 123, 121, 118, 112, 112, 109, 101, 101, 101, 93, 89, 87, 86, 83, 82, 80, 77, 77, 76, 74, 71, 70, 69, 69, 68, 66, 65, 63, 62, 61, 59, 59, 59, 57, 56, 56, 55, 55, 54, 54, 52, 52, 51, 51, 49, 49, 48, 48, 48, 47, 47, 46, 46, 46, 45, 44, 44, 44, 44, 44, 44, 44, 43, 43, 43, 42, 42, 42, 41, 41, 40, 40, 40, 40, 40, 40, 40, 39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 37, 37, 37, 37, 36, 36, 35, 35, 35, 35, 35, 35, 35, 35, 34, 34, 34, 33, 33, 33, 33, 32, 32, 32, 32, 32, 32, 31, 31, 31, 30, 30, 30, 30, 29, 29, 29, 29, 28, 28, 28, 28, 28, 28, 28, 28, 27, 27, 27, 27, 26, 26, 26, 26, 26, 26, 26, 26, 26, 25, 25, 25, 25, 25, 24, 24, 24, 24, 23, 23, 23, 23, 22, 22, 22, 21, 21, 21, 21, 21, 21, 21, 21, 21, 20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 9, 9, 9, 9, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 5, 5, 5, 5, 4, 4, 3, 3, 3, 3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "clusters = do_clustering(test_fps, threshold = 0.5)\n",
    "print('NUM CLUSTERS:', len(clusters))\n",
    "clusters = {cluster_idx: cluster for cluster_idx, cluster in enumerate(sorted(clusters, key = lambda x: -len(x)))}\n",
    "print(clusters)\n",
    "print([len(v) for v in clusters.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecdd4a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10990, 1447, 11268, 11023, 10329, 10959, 10935, 6349, 11002, 4685]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickup_cluster_elements(clusters[0], all_fps=test_fps, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "378719fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(v) for v in clusters.values()], log = True, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "050e955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12\n",
      "1 11\n",
      "2 10\n",
      "3 6\n",
      "4 6\n",
      "5 6\n",
      "6 5\n",
      "7 4\n",
      "8 4\n",
      "9 4\n",
      "10 4\n",
      "11 4\n",
      "12 4\n",
      "13 4\n",
      "14 3\n",
      "15 3\n",
      "16 3\n",
      "17 3\n",
      "18 3\n",
      "19 3\n",
      "20 3\n",
      "21 3\n",
      "22 2\n",
      "23 2\n",
      "24 2\n",
      "25 2\n",
      "26 2\n",
      "27 2\n",
      "28 2\n",
      "29 2\n",
      "30 2\n",
      "31 2\n",
      "32 2\n",
      "33 2\n",
      "34 2\n",
      "35 2\n",
      "36 2\n",
      "37 2\n",
      "38 2\n",
      "39 2\n",
      "40 2\n",
      "41 2\n",
      "42 1\n",
      "43 1\n",
      "44 1\n",
      "45 1\n",
      "46 1\n",
      "47 1\n",
      "48 1\n",
      "49 1\n",
      "50 1\n",
      "51 1\n",
      "52 1\n",
      "53 1\n",
      "54 1\n",
      "55 1\n",
      "56 1\n",
      "57 1\n",
      "58 1\n",
      "59 1\n",
      "60 1\n",
      "61 1\n",
      "62 1\n",
      "63 1\n",
      "64 1\n",
      "65 1\n",
      "66 1\n",
      "67 1\n",
      "68 1\n",
      "69 1\n",
      "70 1\n",
      "71 1\n",
      "72 1\n",
      "73 1\n",
      "74 1\n",
      "75 1\n",
      "76 1\n",
      "77 1\n",
      "78 1\n",
      "79 1\n",
      "80 1\n",
      "81 1\n",
      "82 1\n",
      "83 1\n",
      "84 1\n",
      "85 1\n",
      "86 1\n",
      "87 1\n",
      "88 1\n",
      "89 1\n",
      "90 1\n",
      "91 1\n",
      "92 1\n",
      "93 1\n",
      "94 1\n",
      "95 1\n",
      "96 1\n",
      "97 1\n",
      "98 1\n",
      "99 1\n",
      "100 1\n",
      "101 1\n",
      "102 1\n",
      "103 1\n",
      "104 1\n",
      "105 1\n",
      "106 1\n",
      "107 1\n",
      "108 1\n",
      "109 1\n",
      "110 1\n",
      "111 1\n",
      "112 1\n",
      "113 1\n",
      "114 1\n",
      "115 1\n",
      "116 1\n",
      "117 1\n",
      "118 1\n",
      "119 1\n",
      "120 1\n",
      "121 1\n",
      "122 1\n",
      "123 1\n",
      "124 1\n",
      "125 1\n",
      "126 1\n",
      "127 1\n",
      "128 1\n",
      "129 1\n",
      "130 1\n",
      "131 1\n",
      "132 1\n",
      "133 1\n",
      "134 1\n",
      "135 1\n",
      "136 1\n",
      "137 1\n",
      "138 1\n",
      "139 1\n",
      "140 1\n",
      "141 1\n",
      "142 1\n",
      "143 1\n",
      "144 1\n",
      "145 1\n",
      "146 1\n",
      "147 1\n",
      "148 1\n",
      "149 1\n",
      "150 1\n",
      "151 1\n",
      "152 1\n",
      "153 1\n",
      "154 1\n",
      "155 1\n",
      "156 1\n",
      "157 1\n",
      "158 1\n",
      "159 1\n",
      "160 1\n",
      "161 1\n",
      "162 1\n",
      "163 1\n",
      "164 1\n",
      "165 1\n",
      "166 1\n",
      "167 1\n",
      "168 1\n",
      "169 1\n",
      "170 1\n",
      "171 1\n",
      "172 1\n",
      "173 1\n",
      "174 1\n",
      "175 1\n",
      "176 1\n",
      "177 1\n",
      "178 1\n",
      "179 1\n",
      "180 1\n",
      "181 1\n",
      "182 1\n",
      "183 1\n",
      "184 1\n",
      "185 1\n",
      "186 1\n",
      "187 1\n",
      "188 1\n",
      "189 1\n",
      "190 1\n",
      "191 1\n",
      "192 1\n",
      "193 1\n",
      "194 1\n",
      "195 1\n",
      "196 1\n",
      "197 1\n",
      "198 1\n",
      "199 1\n",
      "200 1\n",
      "201 1\n",
      "202 1\n",
      "203 1\n",
      "204 1\n",
      "205 1\n",
      "206 1\n",
      "207 1\n",
      "208 1\n",
      "209 1\n",
      "210 1\n",
      "211 1\n",
      "212 1\n",
      "213 1\n",
      "214 1\n",
      "215 1\n",
      "216 1\n",
      "217 1\n",
      "218 1\n",
      "219 1\n",
      "220 1\n",
      "221 1\n",
      "222 1\n",
      "223 1\n",
      "224 1\n",
      "225 1\n",
      "226 1\n",
      "227 1\n",
      "228 1\n",
      "229 1\n",
      "230 1\n",
      "231 1\n",
      "232 1\n",
      "233 1\n",
      "234 1\n",
      "235 1\n",
      "236 1\n",
      "237 1\n",
      "238 1\n",
      "239 1\n",
      "240 1\n",
      "241 1\n",
      "242 1\n",
      "243 1\n",
      "244 1\n",
      "245 1\n",
      "246 1\n",
      "247 1\n",
      "248 1\n",
      "249 1\n",
      "250 1\n",
      "251 1\n",
      "252 1\n",
      "253 1\n",
      "254 1\n",
      "255 1\n",
      "256 1\n",
      "257 1\n",
      "258 1\n",
      "259 1\n",
      "260 1\n",
      "261 1\n",
      "262 1\n",
      "263 1\n",
      "264 1\n",
      "265 1\n",
      "266 1\n",
      "267 1\n",
      "268 1\n",
      "269 1\n",
      "270 1\n",
      "271 1\n",
      "272 1\n",
      "273 1\n",
      "274 1\n",
      "275 1\n",
      "276 1\n",
      "277 1\n",
      "278 1\n",
      "279 1\n",
      "280 1\n",
      "281 1\n",
      "282 1\n",
      "283 1\n",
      "284 1\n",
      "285 1\n",
      "286 1\n",
      "287 1\n",
      "288 1\n",
      "289 1\n",
      "290 1\n",
      "291 1\n",
      "NUM SELECTED: 398\n",
      "[10990, 1447, 11268, 11023, 10329, 10959, 10935, 6349, 11002, 4685, 11162, 10614, 8152, 478, 8124, 7920, 2844, 4171, 2528, 9016, 6858, 3674, 9745, 5742, 5348, 5079, 5326, 5541, 3753, 5388, 5424, 5676, 5557, 6433, 1001, 6005, 10411, 8859, 9747, 5989, 1095, 4036, 8995, 1841, 9308, 2963, 671, 2690, 1637, 3304, 8852, 3053, 1057, 9253, 4047, 3716, 3034, 784, 2983, 1394, 3073, 786, 3201, 4399, 2714, 720, 2739, 8030, 827, 1035, 2153, 6716, 3298, 3961, 9250, 7938, 2544, 2010, 2563, 2646, 2914, 756, 2702, 4240, 6171, 874, 6329, 972, 7256, 1102, 10494, 10372, 4527, 9153, 2735, 9179, 7162, 698, 7190, 1185, 953, 10467, 3867, 3328, 9834, 5077, 1586, 5063, 3160, 1817, 1048, 6870, 543, 483, 995, 6970, 1303, 6734, 1061, 6859, 11152, 10709, 3766, 3628, 7085, 1233, 2444, 2295, 3948, 1515, 1715, 1837, 7064, 1107, 9953, 1894, 5188, 410, 10325, 10446, 4699, 4714, 5277, 1957, 7006, 1150, 6544, 2782, 931, 6057, 4847, 2991, 3595, 2271, 8524, 492, 3819, 10118, 5371, 8974, 379, 4046, 7680, 6443, 7211, 5357, 4953, 948, 2817, 5864, 7367, 7924, 4735, 6852, 3406, 8275, 437, 9855, 2255, 7594, 1874, 10312, 1825, 10674, 3322, 9493, 10859, 4396, 9073, 3095, 4497, 5207, 4530, 4913, 4129, 6950, 86, 9824, 9663, 9995, 7764, 1686, 10708, 8064, 4606, 203, 1387, 7415, 4561, 5578, 10020, 5081, 6525, 7846, 8362, 470, 315, 4955, 8391, 3388, 9353, 10570, 6259, 2000, 9787, 9620, 8669, 6425, 5481, 2361, 7889, 2466, 11115, 414, 5227, 5313, 3742, 2325, 1916, 5091, 3490, 10222, 9526, 7005, 2708, 5423, 482, 270, 2163, 9470, 4411, 6019, 3929, 10515, 622, 5892, 5769, 8375, 8836, 3974, 8417, 10261, 9875, 4386, 11173, 2054, 3609, 3568, 8208, 1237, 9194, 5986, 10556, 6786, 6308, 5817, 4324, 2656, 8506, 7732, 1120, 11126, 6146, 8454, 10546, 7646, 7519, 2598, 1767, 1224, 11056, 2891, 8768, 4997, 8310, 8127, 9367, 11012, 8752, 394, 1649, 11212, 4487, 2633, 2457, 821, 2952, 3258, 4824, 3430, 10404, 7621, 7493, 8943, 10637, 6107, 5914, 4687, 350, 9314, 4201, 1964, 1882, 10756, 1723, 9145, 5298, 8203, 4243, 4140, 2491, 745, 9210, 5400, 6186, 7818, 7431, 1479, 7136, 3228, 10365, 9405, 4257, 1623, 11253, 3004, 10174, 2214, 7257, 5626, 4103, 3917, 9587, 5819, 8597, 3642, 8092, 9429, 10832, 10033, 7971, 1737, 2789, 8086, 2081, 246, 184, 6601, 8804, 7319, 9284, 1945, 1553, 8811, 5524, 157, 7374, 1457, 5258, 2028, 1433, 1356, 9269, 4550, 9116, 4873, 4003, 9938, 2651, 1346, 7945, 6630, 5735, 3353, 616, 4639, 4668, 8617]\n"
     ]
    }
   ],
   "source": [
    "selected_idxs = []\n",
    "for cluster_idx, cluster in clusters.items():\n",
    "    n = max(1, len(cluster) // 30)\n",
    "    print(cluster_idx, n)\n",
    "    selected_idxs.extend(pickup_cluster_elements(cluster, test_fps, n = n))\n",
    "print('NUM SELECTED:', len(selected_idxs))\n",
    "print(selected_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89c4c1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 2048)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_features = test_features[selected_idxs]\n",
    "background_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08762538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dbeeee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, features, backgrounds, method = 'gradient', device = 'cuda:0'):\n",
    "    device = torch.device(device)\n",
    "    if method == 'gradient':\n",
    "        features_tensor = torch.from_numpy(features).float().to(device)\n",
    "        background_tensor = torch.from_numpy(backgrounds).float().to(device)\n",
    "        explainer = shap.GradientExplainer(model.to(device).eval(),\n",
    "                                           background_tensor,\n",
    "                                           batch_size=4096,\n",
    "                                           local_smoothing=0)\n",
    "        shap_values = explainer.shap_values(features_tensor, nsamples=200)\n",
    "        if len(shap_values.shape) > 2:\n",
    "            old_shape = shap_values.shape\n",
    "            shap_values = shap_values[..., 0]\n",
    "            print(old_shape, '-->', shap_values.shape)\n",
    "            \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    feat_imp_df = get_feature_importance(shap_values, [f'ECFP6_{i}' for i in range(2048)])\n",
    "    print('SUM:', feat_imp_df['importance'].sum())\n",
    "    print('NUMBER OF > 0:', len(feat_imp_df.query('importance > 0')))\n",
    "    \n",
    "    return shap_values, feat_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ba6f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 µs, sys: 1 µs, total: 14 µs\n",
      "Wall time: 26.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if DEBUG:\n",
    "    model = get_mlp_model(ckpt_paths[0], class_idx = 1, device = DEVICE)\n",
    "    print(model)\n",
    "    shap_values, feat_imp_df = analyze_feature_importance(model, test_features[:50], background_features)\n",
    "    display(feat_imp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f271e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "407d0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_fold(fold_idx, ckpt_path, class_idx, features, backgrounds, save_root_dir, device = DEVICE):\n",
    "    print(f'---Processing fold {fold_idx}---')\n",
    "    model = get_mlp_model(ckpt_path, class_idx = class_idx, device = device)\n",
    "    print(model)\n",
    "    start = time.time()\n",
    "    shap_values, feat_imp_df = analyze_feature_importance(model, features, backgrounds,\n",
    "                                                          method = 'gradient', device = device)\n",
    "    end = time.time()\n",
    "    print('TAKE TIME:', fold_idx, end - start, 's')\n",
    "    save_imp_dir = os.path.join(save_root_dir, str(class_idx), 'importance')\n",
    "    save_shap_dir = os.path.join(save_root_dir, str(class_idx), 'shap')\n",
    "    os.makedirs(save_imp_dir, exist_ok=True)\n",
    "    os.makedirs(save_shap_dir, exist_ok=True)\n",
    "    np.save(os.path.join(save_shap_dir, f'{fold_idx}.npy'), shap_values)\n",
    "    feat_imp_df.to_csv(os.path.join(save_imp_dir, f'{fold_idx}.csv'), index = False)\n",
    "    return shap_values, feat_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2db6843e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11271, 2048), (398, 2048))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape, background_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7debc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73329404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd25a649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cuda:0', 'cuda:0', 'cuda:0', 'cuda:0', 'cuda:0', 'cuda:0', 'cuda:0', 'cuda:0', 'cuda:0', 'cuda:0', 'cuda:1', 'cuda:1', 'cuda:1', 'cuda:1', 'cuda:1', 'cuda:1', 'cuda:1', 'cuda:1', 'cuda:1', 'cuda:1', 'cuda:2', 'cuda:2', 'cuda:2', 'cuda:2', 'cuda:2', 'cuda:2', 'cuda:2', 'cuda:2', 'cuda:2', 'cuda:2', 'cuda:3', 'cuda:3', 'cuda:3', 'cuda:3', 'cuda:3', 'cuda:3', 'cuda:3', 'cuda:3', 'cuda:3', 'cuda:3']\n"
     ]
    }
   ],
   "source": [
    "devices = ['cuda:0'] * 10 + ['cuda:1'] * 10 + ['cuda:2'] * 10 + ['cuda:3'] * 10\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3570bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    shap_values, feat_imp_df = process_one_fold(0,\n",
    "                     ckpt_paths[0],\n",
    "                     class_idx = CLASS_IDX,\n",
    "                     features=test_features[:50],\n",
    "                     backgrounds=background_features,\n",
    "                    save_root_dir = SAVE_DIR,\n",
    "                    device = 'cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9399559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 40/40 [00:01<00:00, 22.99it/s]\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Processing fold 28---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 92.12654185043674\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 28 1599.623901605606 s\n",
      "---Processing fold 29---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 104.43575843145892\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 29 1608.9987320899963 s\n",
      "---Processing fold 26---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 85.83431083718662\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 26 1612.9722001552582 s\n",
      "---Processing fold 21---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 84.38881578918455\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 21 1619.9323935508728 s\n",
      "---Processing fold 7---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 73.47598976691526\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 7 1635.4858527183533 s\n",
      "---Processing fold 38---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 69.92282749284024\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 38 1678.970026731491 s\n",
      "---Processing fold 31---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 84.84032782658706\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 31 1683.1052486896515 s\n",
      "---Processing fold 30---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 85.40303776942153\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 30 1692.0497472286224 s\n",
      "---Processing fold 39---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 79.6866509182258\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 39 1685.6068370342255 s\n",
      "---Processing fold 34---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 70.40218469655704\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 34 1696.3078927993774 s\n",
      "---Processing fold 32---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 61.20893875171155\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 32 1760.0938642024994 s\n",
      "---Processing fold 27---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 69.78043097213336\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 27 1772.871969461441 s\n",
      "---Processing fold 25---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 66.2965393662474\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 25 2020.7735772132874 s\n",
      "---Processing fold 4---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 72.61531655873814\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 4 2063.2050733566284 s\n",
      "---Processing fold 22---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 97.77943690692761\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 22 2175.236991405487 s\n",
      "---Processing fold 1---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 54.60316220195253\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 1 2194.792557477951 s\n",
      "---Processing fold 20---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 74.15685048804812\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 20 2203.916510105133 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Processing fold 2---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 61.333226433718536\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 2 2230.5193009376526 s\n",
      "---Processing fold 23---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 79.06273784714128\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 23 2232.960779905319 s\n",
      "---Processing fold 6---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 64.17211364886751\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 6 2259.9258155822754 s\n",
      "---Processing fold 3---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 58.85774967400769\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 3 2251.1999287605286 s\n",
      "---Processing fold 33---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 59.34279436232046\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 33 2265.890863418579 s\n",
      "---Processing fold 5---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 79.6934666338864\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 5 2259.1791484355927 s\n",
      "---Processing fold 37---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 78.24422380118736\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 37 2284.692186832428 s\n",
      "---Processing fold 9---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 53.48220792855273\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 9 2362.521654367447 s\n",
      "---Processing fold 0---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 64.1912478219164\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 0 2404.1304898262024 s\n",
      "---Processing fold 8---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 69.01885224805314\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 8 2401.6363463401794 s\n",
      "---Processing fold 36---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 97.39146506586226\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 36 2418.572902441025 s\n",
      "---Processing fold 35---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 61.279813106431604\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 35 2405.3652284145355 s\n",
      "---Processing fold 24---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 74.13819348124919\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 24 2417.72930765152 s\n",
      "---Processing fold 17---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 58.71712603649115\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 17 6033.3975303173065 s\n",
      "---Processing fold 15---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 74.61358618305204\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 15 6085.241472005844 s\n",
      "---Processing fold 13---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 82.75307706261873\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 13 6107.309376955032 s\n",
      "Processing class 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 40/40 [00:06<00:00,  5.85it/s]\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Processing fold 24---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 52.26982735599137\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 24 1632.3542647361755 s\n",
      "---Processing fold 38---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 40.755449079770436\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 38 1646.2633345127106 s\n",
      "---Processing fold 33---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 50.61318332107796\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 33 1638.3874397277832 s\n",
      "---Processing fold 37---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 34.66203925123041\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 37 1676.3783371448517 s\n",
      "---Processing fold 19---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 57.20138121571267\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 19 6247.437482118607 s\n",
      "---Processing fold 0---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 45.30094179238964\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 0 1791.4579870700836 s\n",
      "---Processing fold 12---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 68.03404612210944\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 12 6388.19655418396 s\n",
      "---Processing fold 3---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 56.425023508950154\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 3 1792.6210548877716 s\n",
      "---Processing fold 16---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 71.34070094582972\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 16 6438.872345685959 s\n",
      "---Processing fold 5---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 45.1252449573667\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 5 1797.9991388320923 s\n",
      "---Processing fold 18---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 63.84178717565766\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 18 6455.096321344376 s\n",
      "---Processing fold 6---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 49.664279011654656\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 6 1802.4268848896027 s\n",
      "---Processing fold 11---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 54.791979024446036\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 11 6379.75525021553 s\n",
      "---Processing fold 2---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 52.44069480301576\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 2 1804.8692047595978 s\n",
      "---Processing fold 10---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 55.040779336669324\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 10 6308.574415206909 s\n",
      "---Processing fold 1---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 42.30675344086463\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 1 1808.5224778652191 s\n",
      "---Processing fold 9---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 45.23638480585231\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 9 1825.4958860874176 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Processing fold 7---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 48.67351627394754\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 7 1845.948725938797 s\n",
      "---Processing fold 22---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 53.34164542188267\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 22 2201.785763025284 s\n",
      "---Processing fold 14---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 75.99616749062608\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 14 6418.019713401794 s\n",
      "---Processing fold 4---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 57.40958369170918\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 4 2359.006957769394 s\n",
      "---Processing fold 32---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 51.30840035505109\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 32 2298.623500108719 s\n",
      "---Processing fold 28---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 64.87723803916867\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 28 2420.115860939026 s\n",
      "---Processing fold 39---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 56.716640185898754\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 39 2494.9216611385345 s\n",
      "---Processing fold 20---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 58.95117324928433\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 20 2505.001050233841 s\n",
      "---Processing fold 23---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 61.39653392824256\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 23 2510.8867633342743 s\n",
      "---Processing fold 35---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 53.45519737433477\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 35 2491.7602832317352 s\n",
      "---Processing fold 25---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 52.052393190918686\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 25 2512.508967399597 s\n",
      "---Processing fold 8---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 47.76725922953314\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 8 2524.675882577896 s\n",
      "---Processing fold 26---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 52.216258599402074\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 26 2522.7080340385437 s\n",
      "---Processing fold 34---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 50.21445805286646\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 34 2536.4237036705017 s\n",
      "---Processing fold 31---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 60.1991205498876\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 31 2509.2550172805786 s\n",
      "---Processing fold 29---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 38.80956885217293\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 29 2523.129762649536 s\n",
      "---Processing fold 21---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 52.83923151366248\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 21 2544.4873855113983 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Processing fold 27---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 57.74637829237641\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 27 2538.3814237117767 s\n",
      "---Processing fold 36---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 65.84040691296566\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 36 2546.6289613246918 s\n",
      "---Processing fold 30---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 44.49664229903798\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 30 2540.651221036911 s\n",
      "---Processing fold 16---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 51.68820252796168\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 16 5410.747898578644 s\n",
      "---Processing fold 11---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 55.447058605122585\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 11 5449.954791069031 s\n",
      "---Processing fold 10---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 50.472123825312195\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 10 6123.679051160812 s\n",
      "---Processing fold 14---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 45.38612460263363\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 14 6380.3438313007355 s\n",
      "---Processing fold 15---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 49.6475963405853\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 15 6484.196497678757 s\n",
      "---Processing fold 19---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 54.707759599614235\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 19 6522.927823781967 s\n",
      "Processing class 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 40/40 [00:14<00:00,  2.71it/s]\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "/tmp/ipykernel_59638/1858804327.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987394225/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Processing fold 17---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 52.57212419749519\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 17 6727.289125919342 s\n",
      "---Processing fold 0---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 70.84822974556938\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 0 1546.1295218467712 s\n",
      "---Processing fold 12---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 54.11782230022641\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 12 6923.410435438156 s\n",
      "---Processing fold 1---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 57.24405149503519\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 1 1547.9010775089264 s\n",
      "---Processing fold 18---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 54.0192301322725\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 18 6994.710082530975 s\n",
      "---Processing fold 2---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 68.90618022510138\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 2 1571.2594211101532 s\n",
      "---Processing fold 13---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 48.44072529129549\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 13 6997.264889240265 s\n",
      "---Processing fold 3---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 84.92287085664177\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 3 1571.7692606449127 s\n",
      "---Processing fold 29---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 102.6519457768384\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 29 1549.2176203727722 s\n",
      "---Processing fold 35---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 97.81099529947174\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 35 1549.9451491832733 s\n",
      "---Processing fold 32---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 84.16414318495495\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 32 1565.8894231319427 s\n",
      "---Processing fold 28---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 123.70895912092367\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 28 1890.7446100711823 s\n",
      "---Processing fold 7---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 88.33432334571896\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 7 2291.962486743927 s\n",
      "---Processing fold 38---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 85.23750645294744\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 38 2351.5278046131134 s\n",
      "---Processing fold 20---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 107.64106524717918\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 20 2346.7419006824493 s\n",
      "---Processing fold 6---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 79.94332559895797\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 6 2351.7235991954803 s\n",
      "---Processing fold 21---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 117.36483477755337\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 21 2372.5562205314636 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Processing fold 37---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 87.2416876418527\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 37 2428.272102355957 s\n",
      "---Processing fold 23---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 86.91788516752781\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 23 2444.4709668159485 s\n",
      "---Processing fold 30---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 96.34142388136804\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 30 2515.774756669998 s\n",
      "---Processing fold 33---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 70.1201482238258\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 33 2515.458693265915 s\n",
      "---Processing fold 8---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 89.3452665468177\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 8 2546.4554193019867 s\n",
      "---Processing fold 5---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 75.4709397396681\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 5 2549.4445612430573 s\n",
      "---Processing fold 22---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 99.93972781695567\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 22 2527.6041209697723 s\n",
      "---Processing fold 4---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 93.90154277549439\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 4 2554.5897374153137 s\n",
      "---Processing fold 36---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 117.877185244684\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 36 2538.105541706085 s\n",
      "---Processing fold 9---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 77.00364306353194\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 9 2560.40798330307 s\n",
      "---Processing fold 24---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 86.07722615344599\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 24 2578.025208711624 s\n",
      "---Processing fold 27---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 92.474140681299\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 27 2571.441111087799 s\n",
      "---Processing fold 34---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 90.62163378066217\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 34 2559.5096690654755 s\n",
      "---Processing fold 39---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 95.61782636618106\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 39 2581.312652826309 s\n",
      "---Processing fold 25---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 96.69300744791181\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 25 2585.080130815506 s\n",
      "---Processing fold 31---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 90.5275260144157\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 31 2579.762809276581 s\n",
      "---Processing fold 26---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 93.59718945640405\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 26 2595.8680279254913 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Processing fold 16---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 73.83467667746133\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 16 5412.995553970337 s\n",
      "---Processing fold 10---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 81.36907381026879\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 10 6490.46519613266 s\n",
      "---Processing fold 13---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 100.69892798237196\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 13 6703.666709423065 s\n",
      "---Processing fold 18---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 86.81664819073336\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 18 6708.930853128433 s\n",
      "---Processing fold 14---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 95.31207440905847\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 14 6773.330775976181 s\n",
      "---Processing fold 12---\n",
      "MlpModel(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "(11271, 2048, 1) --> (11271, 2048)\n",
      "SUM: 105.32669186002678\n",
      "NUMBER OF > 0: 822\n",
      "TAKE TIME: 12 6811.204345464706 s\n",
      "[40, 40, 40]\n"
     ]
    }
   ],
   "source": [
    "ret = {}\n",
    "for class_idx in CLASS_IDXS:\n",
    "    print('Processing class', class_idx)\n",
    "    ret[class_idx] = Parallel(n_jobs=len(ckpt_paths), backend='loky')(\n",
    "        delayed(process_one_fold)(i,\n",
    "                                 ckpt_paths[i],\n",
    "                                 class_idx = class_idx,\n",
    "                                 features = test_features,\n",
    "                                 backgrounds = background_features,\n",
    "                                 save_root_dir = SAVE_DIR,\n",
    "                                 device = devices[i])\n",
    "                for i in tqdm(range(len(ckpt_paths))))\n",
    "print([len(v) for v in ret.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33972542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r /home/dangnh36/datasets/competitions/leash_belka/processed/feature_importance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466329f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d704f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36feb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
