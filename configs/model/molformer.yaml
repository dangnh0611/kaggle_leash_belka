_target_: src.modules.models.molformer.MolFormer

model_path: /root/.cache/huggingface/hub/models--ibm--MoLFormer-XL-both-10pct/snapshots/7b12d946c181a37f6012b9dc3b002275de070314/
vocab_size: 224
pool_type: masked_avg

num_trainable_layers: 0

# Molformer default
# encoder:
#   vocab_size: ${model.vocab_size}
#   hidden_size: 768
#   num_hidden_layers: 12
#   num_attention_heads: 12
#   intermediate_size: 768
#   hidden_act: gelu
#   hidden_dropout_prob: 0.1
#   embedding_dropout_prob: 0.2
#   max_position_embeddings: 150
#   initializer_range: 0.02
#   layer_norm_eps: 1e-12
#   linear_attention_eps: 1e-6
#   num_random_features: 32
#   feature_map_kernel: relu
#   pad_token_id: 0

encoder: {}

# encoder:
#   vocab_size: ${model.vocab_size}
#   hidden_size: 128
#   num_hidden_layers: 6
#   num_attention_heads: 4
#   intermediate_size: 256
#   hidden_act: gelu
#   hidden_dropout_prob: 0.1
#   embedding_dropout_prob: 0.2
#   max_position_embeddings: 150
#   initializer_range: 0.02
#   layer_norm_eps: 1e-12
#   linear_attention_eps: 1e-6
#   num_random_features: 16
#   feature_map_kernel: relu
#   pad_token_id: 0

# encoder:
#   # vocab_size: ${model.vocab_size}
#   hidden_size: 32
#   num_hidden_layers: 1
#   num_attention_heads: 4
#   intermediate_size: 64
#   hidden_act: gelu
#   hidden_dropout_prob: 0.1
#   embedding_dropout_prob: 0.2
#   max_position_embeddings: 150
#   initializer_range: 0.02
#   layer_norm_eps: 1e-12
#   linear_attention_eps: 1e-6
#   num_random_features: 16
#   feature_map_kernel: relu
#   pad_token_id: 0

head:
  mlp_chans: [512, 512]
  norm: null
  act: gelu
  dropout: 0.3
