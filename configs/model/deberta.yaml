_target_: src.modules.models.deberta.DebertaModel

vocab_size: 224
pool_type: concat_attn

encoder:
  vocab_size: ${model.vocab_size}
  hidden_size: 256
  num_hidden_layers: 8
  num_attention_heads: 4
  intermediate_size: 512
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 256
  position_buckets: 256
  norm_rel_ebd: layer_norm
  share_att_key: True
  pos_att_type: "p2c|c2p"
  type_vocab_size: 0
  initializer_range: 0.02
  layer_norm_eps: 1e-7
  relative_attention: True
  max_relative_positions: -1
  position_biased_input: False
  pad_token_id: 0


head:
  type: leash
  num_output: 3
  mlp_chans: [512, 512]
  norm: null
  act: gelu
  dropout: 0.3
