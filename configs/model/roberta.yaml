_target_: src.modules.models.roberta.RobertaForSequenceClassificationModel

vocab_size: 224
pool_type: concat_attn

encoder:
  vocab_size: ${model.vocab_size}
  hidden_size: 128
  num_hidden_layers: 6
  num_attention_heads: 4
  intermediate_size: 256
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 256
  type_vocab_size: 2
  initializer_range: 0.02
  layer_norm_eps: 1e-12
  position_embedding_type: absolute # `absolute`, `relative_key`, `relative_key_query`
  use_cache: True

head:
  type: leash
  num_output: 3
  mlp_chans: [512, 512]
  norm: null
  act: gelu
  dropout: 0.3
