_target_: src.modules.models.mamba.MambaForSequenceClassificationModel

vocab_size: 224
pool_type: concat_attn

encoder:
  vocab_size: ${model.vocab_size}
  hidden_size: 128
  state_size: 16
  num_hidden_layers: 6
  layer_norm_epsilon: 1e-5
  pad_token_id: 0
  bos_token_id: 0
  eos_token_id: 0
  expand: 2
  conv_kernel: 4
  use_bias: False
  use_conv_bias: True
  hidden_act: silu
  initializer_range: 0.1
  residual_in_fp32: True
  time_step_rank: auto
  time_step_scale: 1.0
  time_step_min: 0.001
  time_step_max: 0.1
  time_step_init_scheme: random
  time_step_floor: 1e-4
  rescale_prenorm_residual: False
  use_cache: True  
  ssm_cfg: {}
  rms_norm: True

head:
  type: leash
  num_output: 3
  mlp_chans: [512, 512]
  norm: null
  act: gelu
  dropout: 0.3
